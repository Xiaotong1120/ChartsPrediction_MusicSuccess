{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee122ff-6991-4f04-b93f-caa4839ce951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/13 18:54:01 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/04/13 18:54:01 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/04/13 18:54:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/04/13 18:54:01 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, when, expr, log, sqrt, pow, abs, udf, lit, round, concat_ws\n",
    "from pyspark.sql.types import DoubleType, BooleanType, StringType, ArrayType\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, VectorSlicer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"\n",
    "Spotify Charts Prediction\n",
    "\"\"\"\n",
    "\n",
    "# Spark Configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spotify Prediction System - Final\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a653f1b6-6bdc-4e02-abbd-8df65bb4718e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Counts:278731\n",
      "On Charts Counts: 7510 (2.69%)\n",
      "Not On Charts Counts: 271221 (97.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# 1. Data Loading and Pre-manipulation\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Data loading\n",
    "data_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_prepared_data_complete\"\n",
    "completed_songs = spark.read.parquet(data_path)\n",
    "\n",
    "# Confirm data types\n",
    "completed_songs = completed_songs.withColumn(\"label\", col(\"is_ranked\").cast(\"double\"))\n",
    "\n",
    "# Statistics\n",
    "total_count = completed_songs.count()\n",
    "positive_count = completed_songs.filter(col(\"label\") == 1.0).count()\n",
    "negative_count = completed_songs.filter(col(\"label\") == 0.0).count()\n",
    "\n",
    "print(f\"Total Counts:{total_count}\")\n",
    "print(f\"On Charts Counts: {positive_count} ({positive_count/total_count*100:.2f}%)\")\n",
    "print(f\"Not On Charts Counts: {negative_count} ({negative_count/total_count*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c89d53-2db2-46ef-812b-3443cfcd2385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# 2. Feature Enhancement\n",
    "#------------------------------------------------------------------------------\n",
    "# Orgin\n",
    "audio_features = [\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \n",
    "                  \"speechiness\", \"acousticness\", \"instrumentalness\", \n",
    "                  \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
    "\n",
    "for feature in audio_features:\n",
    "    completed_songs = completed_songs.withColumn(\n",
    "        feature, col(feature).cast(\"double\")\n",
    "    )\n",
    "\n",
    "#-- Music theory related features --#\n",
    "\n",
    "# Energy to Acoustic Ratio - A measure of how electronic vs. acoustic a song is\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"energy_acoustic_ratio\", \n",
    "    col(\"energy\") / (col(\"acousticness\") + 0.001)\n",
    ")\n",
    "\n",
    "# The product of dance and emotion - happy dance music vs sad dance music\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"dance_valence_product\", \n",
    "    col(\"danceability\") * col(\"valence\")\n",
    ")\n",
    "\n",
    "# Vocal-instrumental balance - Differentiate between vocal-dominant and instrument-dominant songs\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"vocal_instrumental_balance\", \n",
    "    (1 - col(\"instrumentalness\")) / (col(\"instrumentalness\") + 0.001)\n",
    ")\n",
    "\n",
    "#-- Audience Perception Related Features --#\n",
    "\n",
    "# Rhythm perception - combining tempo and danceability\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"rhythm_factor\", \n",
    "    col(\"tempo\") * col(\"danceability\") / 100.0\n",
    ")\n",
    "\n",
    "# Emotional intensity - a combination of valence and energy\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"mood_intensity\", \n",
    "    sqrt(pow(col(\"valence\"), 2) + pow(col(\"energy\"), 2))\n",
    ")\n",
    "\n",
    "# Calm vs. Excitement Factor - Differentiating Calm Songs from Exciting Songs\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"calmness_factor\", \n",
    "    (col(\"acousticness\") * (1 - col(\"energy\")) * (1 - col(\"loudness\") / -60.0)) / 3.0\n",
    ")\n",
    "\n",
    "#-- Category Features --#\n",
    "\n",
    "# Is it an obvious instrumental?\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"is_instrumental\", \n",
    "    when(col(\"instrumentalness\") > 0.5, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Is it a strong rhythm song?\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"is_rhythmic\", \n",
    "    when(col(\"danceability\") > 0.7, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Is it a high energy song?\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"is_energetic\", \n",
    "    when(col(\"energy\") > 0.8, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Is it a happy song?\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"is_happy\", \n",
    "    when(col(\"valence\") > 0.7, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "#-- Nonlinear transformation --#\n",
    "\n",
    "# Logarithmic transformation of acoustic properties - improves skewed distribution\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"log_acousticness\", \n",
    "    log(col(\"acousticness\") + 0.001)\n",
    ")\n",
    "\n",
    "# Logarithmic transformation of instrumental characteristics\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"log_instrumentalness\", \n",
    "    log(col(\"instrumentalness\") + 0.001)\n",
    ")\n",
    "\n",
    "# Loudness squared - emphasizes extreme values\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"loudness_squared\", \n",
    "    pow(col(\"loudness\"), 2)\n",
    ")\n",
    "\n",
    "# Loudness Cubed - further emphasizes extremes\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"loudness_cubic\", \n",
    "    pow(col(\"loudness\"), 3)\n",
    ")\n",
    "\n",
    "#-- Composite Indicator --#\n",
    "\n",
    "# Mainstream Popularity Index - Combining multiple ranking related indicators\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"mainstream_index\", \n",
    "    (col(\"danceability\") * 0.25 + \n",
    "     col(\"energy\") * 0.25 + \n",
    "     col(\"valence\") * 0.2 + \n",
    "     (abs(col(\"loudness\")) / 15.0) * 0.15 + \n",
    "     (1 - col(\"acousticness\")) * 0.15)\n",
    ")\n",
    "\n",
    "# Experimental Index - Non-mainstream feature combinations\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"experimental_index\", \n",
    "    (col(\"instrumentalness\") * 0.3 + \n",
    "     col(\"speechiness\") * 0.2 + \n",
    "     col(\"liveness\") * 0.2 + \n",
    "     (1 - col(\"danceability\")) * 0.15 + \n",
    "     (1 - col(\"valence\")) * 0.15)\n",
    ")\n",
    "\n",
    "# Loudness and Energy Interaction\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"loudness_energy_interaction\", \n",
    "    col(\"loudness_squared\") * col(\"energy\")\n",
    ")\n",
    "\n",
    "# Vocal Clarity\n",
    "completed_songs = completed_songs.withColumn(\n",
    "    \"vocal_clarity\", \n",
    "    (1 - col(\"instrumentalness\")) * col(\"speechiness\") / (col(\"acousticness\") + 0.001)\n",
    ")\n",
    "\n",
    "# Define all new features\n",
    "new_features = [\n",
    "    \"energy_acoustic_ratio\", \"dance_valence_product\", \"vocal_instrumental_balance\",\n",
    "    \"rhythm_factor\", \"mood_intensity\", \"calmness_factor\",\n",
    "    \"is_instrumental\", \"is_rhythmic\", \"is_energetic\", \"is_happy\",\n",
    "    \"log_acousticness\", \"log_instrumentalness\", \"loudness_squared\", \"loudness_cubic\",\n",
    "    \"mainstream_index\", \"experimental_index\", \"loudness_energy_interaction\", \"vocal_clarity\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dddd58f-e081-4f8e-9148-6e9efa803b87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 18:54:27 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# 3. Feature vector preparation\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Creating an extended feature vector\n",
    "all_features = audio_features + new_features\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=all_features,\n",
    "    outputCol=\"extended_features\"\n",
    ")\n",
    "\n",
    "# Applying Vector Assembler\n",
    "df_with_extended_features = vector_assembler.transform(completed_songs)\n",
    "\n",
    "# Standardized features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"extended_features\",\n",
    "    outputCol=\"scaled_extended_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_with_extended_features)\n",
    "df_with_scaled_features = scaler_model.transform(df_with_extended_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d286dcc-addf-4af2-8eed-6d45397e7473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 223011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set: 55720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charts songs on the training set: 6065 (2.72%)\n",
      "Class imbalance ratio: 35.77:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:2 Balanced Data Set: 18291 tracks (actual ratio 2.02:1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:3 Balanced Data Set: 24364 tracks (actual ratio 3.02:1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:5 Balanced Data Set: 36575 tracks (actual ratio 5.03:1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# 4. Data partitioning and balanced sampling\n",
    "#------------------------------------------------------------------------------\n",
    "# Split the training set into test set\n",
    "train_data, test_data = df_with_scaled_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Separating the majority and minority classes\n",
    "majority_class = train_data.filter(col(\"label\") == 0.0)\n",
    "minority_class = train_data.filter(col(\"label\") == 1.0)\n",
    "\n",
    "# Number of minority and majority classes\n",
    "minority_count = minority_class.count()\n",
    "majority_count = majority_class.count()\n",
    "class_ratio = majority_count / minority_count\n",
    "\n",
    "print(f\"Training Set: {train_data.count()}\")\n",
    "print(f\"Testing Set: {test_data.count()}\")\n",
    "print(f\"Charts songs on the training set: {minority_count} ({minority_count/train_data.count()*100:.2f}%)\")\n",
    "print(f\"Class imbalance ratio: {class_ratio:.2f}:1\")\n",
    "\n",
    "# Create three datasets with different balance ratios\n",
    "sampling_ratios = {\n",
    "    \"1:2\": (minority_count * 2) / majority_count,\n",
    "    \"1:3\": (minority_count * 3) / majority_count,\n",
    "    \"1:5\": (minority_count * 5) / majority_count\n",
    "}\n",
    "\n",
    "balanced_datasets = {}\n",
    "\n",
    "for ratio_name, ratio in sampling_ratios.items():\n",
    "    sampled_majority = majority_class.sample(False, ratio, seed=42)\n",
    "    balanced_data = sampled_majority.unionAll(minority_class)\n",
    "    balanced_datasets[ratio_name] = balanced_data.cache()\n",
    "    \n",
    "    actual_ratio = sampled_majority.count() / minority_count\n",
    "    print(f\"{ratio_name} Balanced Data Set: {balanced_data.count()} tracks (actual ratio {actual_ratio:.2f}:1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f279eafa-fd40-4692-939b-f7fbc1e9754d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training a 1:2 balanced random forest model (high recall)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:20:28 WARN DAGScheduler: Broadcasting large task binary with size 1289.2 KiB\n",
      "25/04/13 19:20:29 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/04/13 19:20:31 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/04/13 19:20:34 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/04/13 19:20:35 WARN DAGScheduler: Broadcasting large task binary with size 1167.0 KiB\n",
      "25/04/13 19:20:37 WARN DAGScheduler: Broadcasting large task binary with size 9.0 MiB\n",
      "25/04/13 19:20:39 WARN DAGScheduler: Broadcasting large task binary with size 1575.0 KiB\n",
      "25/04/13 19:20:42 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:20:47 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:20:52 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:20:57 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:21:01 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:21:06 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Random Forest - 1:2 Balanced (High Recall) Evaluation =====\n",
      "Accuracy: 0.8436\n",
      "Precision: 0.0981\n",
      "Recall: 0.6138\n",
      "F1: 0.1691\n",
      "F2: 0.2992\n",
      "ROC-AUC: 0.8264\n",
      "PR-AUC: 0.1157\n",
      "\n",
      "Confusion matrix:\n",
      "TP: 887 \n",
      "FP: 8157 \n",
      "TN: 46118 \n",
      "FN: 558 \n",
      "\n",
      "The percentage of songs predicted to be on the charts that actually made it to the charts: 9.81%\n",
      "The percentage of songs predicted not to chart that actually charted: 1.20%\n",
      "\n",
      "Training 1:3 balanced random forest model (balanced)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:21:14 WARN DAGScheduler: Broadcasting large task binary with size 1294.1 KiB\n",
      "25/04/13 19:21:15 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/04/13 19:21:17 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/04/13 19:21:19 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/04/13 19:21:21 WARN DAGScheduler: Broadcasting large task binary with size 1255.4 KiB\n",
      "25/04/13 19:21:22 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "25/04/13 19:21:24 WARN DAGScheduler: Broadcasting large task binary with size 1727.9 KiB\n",
      "25/04/13 19:21:27 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:21:31 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:21:35 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:21:38 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:21:42 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:21:46 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 1:3 balanced random forest model (balanced) Evaluation =====\n",
      "Accuracy: 0.9086\n",
      "Precision: 0.1223\n",
      "Recall: 0.4083\n",
      "F1: 0.1882\n",
      "F2: 0.2781\n",
      "ROC-AUC: 0.8242\n",
      "PR-AUC: 0.1117\n",
      "\n",
      "Confusion matrix:\n",
      "TP: 590 \n",
      "FP: 4236 \n",
      "TN: 50039 \n",
      "FN: 855 \n",
      "\n",
      "The percentage of songs predicted to be on the charts that actually made it to the charts: 12.23%\n",
      "The percentage of songs predicted not to chart that actually charted: 1.68%\n",
      "\n",
      "Training a 1:5 balanced random forest model (high accuracy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:21:53 WARN DAGScheduler: Broadcasting large task binary with size 1294.9 KiB\n",
      "25/04/13 19:21:54 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/04/13 19:21:56 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/04/13 19:21:59 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:22:01 WARN DAGScheduler: Broadcasting large task binary with size 1324.0 KiB\n",
      "25/04/13 19:22:03 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB\n",
      "25/04/13 19:22:06 WARN DAGScheduler: Broadcasting large task binary with size 1878.9 KiB\n",
      "25/04/13 19:22:08 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:22:11 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:22:14 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:22:17 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:22:19 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:22:22 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 1:5 balanced random forest model (high accuracy) Evaluation =====\n",
      "Accuracy: 0.9599\n",
      "Precision: 0.1645\n",
      "Recall: 0.1343\n",
      "F1: 0.1479\n",
      "F2: 0.1394\n",
      "ROC-AUC: 0.8234\n",
      "PR-AUC: 0.1162\n",
      "\n",
      "Confusion matrix:\n",
      "TP: 194 \n",
      "FP: 985 \n",
      "TN: 53290 \n",
      "FN: 1251 \n",
      "\n",
      "The percentage of songs predicted to be on the charts that actually made it to the charts: 16.45%\n",
      "The percentage of songs predicted not to chart that actually charted: 2.29%\n",
      "\n",
      "Training GBT model for threshold adjustment\n",
      "\n",
      "Testing different thresholds of GBT model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.05, Precision: 0.0259, Recall: 1.0000, F1: 0.0506, F2: 0.1175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.10, Precision: 0.0329, Recall: 0.9889, F1: 0.0637, F2: 0.1453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.15, Precision: 0.0415, Recall: 0.9467, F1: 0.0796, F2: 0.1767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.20, Precision: 0.0486, Recall: 0.9017, F1: 0.0923, F2: 0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.30, Precision: 0.0629, Recall: 0.7993, F1: 0.1167, F2: 0.2393\n",
      "\n",
      "Best Threshold (F2): 0.30, F1: 0.1167, F2: 0.2393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GBT - Threshold0.30 Evaluation =====\n",
      "Accuracy: 0.6861\n",
      "Precision: 0.0629\n",
      "Recall: 0.7993\n",
      "F1: 0.1167\n",
      "F2: 0.2393\n",
      "ROC-AUC: 0.8141\n",
      "PR-AUC: 0.0973\n",
      "\n",
      "Confusion matrix:\n",
      "TP: 1155 \n",
      "FP: 17201 \n",
      "TN: 37074 \n",
      "FN: 290 \n",
      "\n",
      "The percentage of songs predicted to be on the charts that actually made it to the charts: 6.29%\n",
      "The percentage of songs predicted not to chart that actually charted: 0.78%\n",
      "\n",
      "===== Model summary (sorted by F1 score) =====\n",
      "Model                               Accuracy   Precision  Recall     F1 Score   F2 Score  \n",
      "-------------------------------------------------------------------------------------\n",
      "1:3 balanced random forest model (balanced) 0.9086     0.1223     0.4083     0.1882     0.2781    \n",
      "Random Forest - 1:2 Balanced (High Recall) 0.8436     0.0981     0.6138     0.1691     0.2992    \n",
      "1:5 balanced random forest model (high accuracy) 0.9599     0.1645     0.1343     0.1479     0.1394    \n",
      "GBT - Threshold0.30                 0.6861     0.0629     0.7993     0.1167     0.2393    \n",
      "\n",
      "The 10 most important features:\n",
      "1. loudness_squared: 0.077646\n",
      "2. experimental_index: 0.076807\n",
      "3. vocal_instrumental_balance: 0.075583\n",
      "4. instrumentalness: 0.073485\n",
      "5. danceability: 0.070767\n",
      "6. log_instrumentalness: 0.065379\n",
      "7. loudness: 0.053775\n",
      "8. loudness_energy_interaction: 0.052954\n",
      "9. loudness_cubic: 0.050165\n",
      "10. rhythm_factor: 0.042124\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# 5. Model Training\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Evaluation Function - Calculates classification metrics\n",
    "def evaluate_model(predictions, label_col=\"label\", prediction_col=\"prediction\", model_name=\"Model\", verbose=True):\n",
    "    \"\"\"Calculates classification metrics\"\"\"\n",
    "    # Confusion matrix\n",
    "    tp = predictions.filter((col(prediction_col) == 1.0) & (col(label_col) == 1.0)).count()\n",
    "    fp = predictions.filter((col(prediction_col) == 1.0) & (col(label_col) == 0.0)).count()\n",
    "    tn = predictions.filter((col(prediction_col) == 0.0) & (col(label_col) == 0.0)).count()\n",
    "    fn = predictions.filter((col(prediction_col) == 0.0) & (col(label_col) == 1.0)).count()\n",
    "    \n",
    "    # Calculation indicators\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    f2 = 5 * precision * recall / (4 * precision + recall) if (precision + recall) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # ROC-AUC和PR-AUC\n",
    "    try:\n",
    "        binary_evaluator = BinaryClassificationEvaluator(\n",
    "            labelCol=label_col, \n",
    "            rawPredictionCol=\"rawPrediction\", \n",
    "            metricName=\"areaUnderROC\"\n",
    "        )\n",
    "        auc = binary_evaluator.evaluate(predictions)\n",
    "        \n",
    "        pr_evaluator = BinaryClassificationEvaluator(\n",
    "            labelCol=label_col, \n",
    "            rawPredictionCol=\"rawPrediction\", \n",
    "            metricName=\"areaUnderPR\"\n",
    "        )\n",
    "        pr_auc = pr_evaluator.evaluate(predictions)\n",
    "    except:\n",
    "        auc = None\n",
    "        pr_auc = None\n",
    "    \n",
    "    # Print results (if verbose=True)\n",
    "    if verbose:\n",
    "        print(f\"\\n===== {model_name} Evaluation =====\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1: {f1:.4f}\")\n",
    "        print(f\"F2: {f2:.4f}\")\n",
    "        \n",
    "        if auc is not None:\n",
    "            print(f\"ROC-AUC: {auc:.4f}\")\n",
    "        if pr_auc is not None:\n",
    "            print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "        \n",
    "        print(\"\\nConfusion matrix:\")\n",
    "        print(f\"TP: {tp} \")\n",
    "        print(f\"FP: {fp} \")\n",
    "        print(f\"TN: {tn} \")\n",
    "        print(f\"FN: {fn} \")\n",
    "        \n",
    "        if tp + fp > 0:\n",
    "            print(f\"\\nThe percentage of songs predicted to be on the charts that actually made it to the charts: {tp/(tp+fp)*100:.2f}%\")\n",
    "        if tn + fn > 0:\n",
    "            print(f\"The percentage of songs predicted not to chart that actually charted: {fn/(tn+fn)*100:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"specificity\": specificity,\n",
    "        \"auc\": auc if auc is not None else 0,\n",
    "        \"pr_auc\": pr_auc if pr_auc is not None else 0,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"tn\": tn,\n",
    "        \"fn\": fn\n",
    "    }\n",
    "\n",
    "# 1:2 balanced random forest model (high recall)\n",
    "print(\"\\nTraining a 1:2 balanced random forest model (high recall)\")\n",
    "rf_high_recall = RandomForestClassifier(\n",
    "    featuresCol=\"scaled_extended_features\", \n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_high_recall_model = rf_high_recall.fit(balanced_datasets[\"1:2\"])\n",
    "rf_high_recall_preds = rf_high_recall_model.transform(test_data)\n",
    "rf_high_recall_metrics = evaluate_model(\n",
    "    rf_high_recall_preds, \n",
    "    model_name=\"Random Forest - 1:2 Balanced (High Recall)\"\n",
    ")\n",
    "\n",
    "# 1:3 balanced random forest model (balanced)\n",
    "print(\"\\nTraining 1:3 balanced random forest model (balanced)\")\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    featuresCol=\"scaled_extended_features\", \n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_balanced_model = rf_balanced.fit(balanced_datasets[\"1:3\"])\n",
    "rf_balanced_preds = rf_balanced_model.transform(test_data)\n",
    "rf_balanced_metrics = evaluate_model(\n",
    "    rf_balanced_preds, \n",
    "    model_name=\"1:3 balanced random forest model (balanced)\"\n",
    ")\n",
    "\n",
    "# 1:5 balanced random forest model (high accuracy)\n",
    "print(\"\\nTraining a 1:5 balanced random forest model (high accuracy)\")\n",
    "rf_high_precision = RandomForestClassifier(\n",
    "    featuresCol=\"scaled_extended_features\", \n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_high_precision_model = rf_high_precision.fit(balanced_datasets[\"1:5\"])\n",
    "rf_high_precision_preds = rf_high_precision_model.transform(test_data)\n",
    "rf_high_precision_metrics = evaluate_model(\n",
    "    rf_high_precision_preds, \n",
    "    model_name=\"1:5 balanced random forest model (high accuracy)\"\n",
    ")\n",
    "\n",
    "# GBT model - for threshold adjustment\n",
    "print(\"\\nTraining GBT model for threshold adjustment\")\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"scaled_extended_features\", \n",
    "    labelCol=\"label\",\n",
    "    maxIter=10,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gbt_model = gbt.fit(balanced_datasets[\"1:2\"])  # Use 1:2 balanced data training\n",
    "gbt_preds = gbt_model.transform(test_data)\n",
    "\n",
    "# Define the threshold adjustment function\n",
    "def create_threshold_udf(threshold):\n",
    "    @udf(returnType=DoubleType())\n",
    "    def apply_threshold(probability):\n",
    "        if probability is None:\n",
    "            return 0.0\n",
    "        return 1.0 if probability[1] > threshold else 0.0\n",
    "    return apply_threshold\n",
    "\n",
    "# Testing different thresholds\n",
    "print(\"\\nTesting different thresholds of GBT model\")\n",
    "thresholds = [0.05, 0.1, 0.15, 0.2, 0.3]\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    threshold_udf = create_threshold_udf(threshold)\n",
    "    threshold_preds = gbt_preds.withColumn(\n",
    "        f\"prediction_{int(threshold*100)}\", \n",
    "        threshold_udf(col(\"probability\"))\n",
    "    )\n",
    "    \n",
    "    tp = threshold_preds.filter(\n",
    "        (col(f\"prediction_{int(threshold*100)}\") == 1.0) & (col(\"label\") == 1.0)\n",
    "    ).count()\n",
    "    fp = threshold_preds.filter(\n",
    "        (col(f\"prediction_{int(threshold*100)}\") == 1.0) & (col(\"label\") == 0.0)\n",
    "    ).count()\n",
    "    tn = threshold_preds.filter(\n",
    "        (col(f\"prediction_{int(threshold*100)}\") == 0.0) & (col(\"label\") == 0.0)\n",
    "    ).count()\n",
    "    fn = threshold_preds.filter(\n",
    "        (col(f\"prediction_{int(threshold*100)}\") == 0.0) & (col(\"label\") == 1.0)\n",
    "    ).count()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    f2 = 5 * precision * recall / (4 * precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, F2: {f2:.4f}\")\n",
    "    threshold_results.append((threshold, precision, recall, f1, f2, tp, fp, tn, fn))\n",
    "\n",
    "# Choose the best threshold (by F2 score, since we prefer recall)\n",
    "best_threshold_result = max(threshold_results, key=lambda x: x[4])\n",
    "best_threshold = best_threshold_result[0]\n",
    "print(f\"\\nBest Threshold (F2): {best_threshold:.2f}, F1: {best_threshold_result[3]:.4f}, F2: {best_threshold_result[4]:.4f}\")\n",
    "\n",
    "# Applying the best threshold\n",
    "best_threshold_udf = create_threshold_udf(best_threshold)\n",
    "gbt_optimized_preds = gbt_preds.withColumn(\n",
    "    \"optimized_prediction\", \n",
    "    best_threshold_udf(col(\"probability\"))\n",
    ")\n",
    "\n",
    "gbt_metrics = evaluate_model(\n",
    "    gbt_optimized_preds,\n",
    "    prediction_col=\"optimized_prediction\",\n",
    "    model_name=f\"GBT - Threshold{best_threshold:.2f}\"\n",
    ")\n",
    "\n",
    "# Aggregate all model results\n",
    "all_models = [\n",
    "    rf_high_recall_metrics,\n",
    "    rf_balanced_metrics,\n",
    "    rf_high_precision_metrics,\n",
    "    gbt_metrics\n",
    "]\n",
    "\n",
    "# Sort by F1 score\n",
    "all_models.sort(key=lambda x: x[\"f1\"], reverse=True)\n",
    "\n",
    "print(\"\\n===== Model summary (sorted by F1 score) =====\")\n",
    "header = \"{:<35} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "   \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"F2 Score\"\n",
    ")\n",
    "print(header)\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for m in all_models:\n",
    "    row = \"{:<35} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
    "        m[\"model\"], \n",
    "        m[\"accuracy\"], \n",
    "        m[\"precision\"], \n",
    "        m[\"recall\"], \n",
    "        m[\"f1\"],\n",
    "        m[\"f2\"]\n",
    "    )\n",
    "    print(row)\n",
    "\n",
    "# Save feature importance\n",
    "feature_importances = []\n",
    "for i, importance in enumerate(rf_balanced_model.featureImportances):\n",
    "    if i < len(all_features):\n",
    "        feature_name = all_features[i]\n",
    "        feature_importances.append((feature_name, importance))\n",
    "\n",
    "feature_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nThe 10 most important features:\")\n",
    "for i, (feature, importance) in enumerate(feature_importances[:10]):\n",
    "    print(f\"{i+1}. {feature}: {importance:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90245b84-6a5e-4364-b232-13085ce9b56a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the tiered prediction system...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, expr, log, sqrt, pow, abs, udf, lit, round, concat_ws, array_position\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, BooleanType, StringType, ArrayType\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, VectorSlicer\n",
    "\n",
    "# Create tiered prediction function (Fixed version)\n",
    "\n",
    "def create_tiered_predictions(\n",
    "    test_data, \n",
    "    high_recall_model, \n",
    "    balanced_model, \n",
    "    high_precision_model, \n",
    "    gbt_model, \n",
    "    gbt_threshold=0.15\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a tiered prediction system that combines predictions from multiple models.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. High-recall model predictions\n",
    "    high_recall_preds = high_recall_model.transform(test_data)\n",
    "\n",
    "    # 2. Balanced model predictions\n",
    "    balanced_preds = balanced_model.transform(test_data)\n",
    "\n",
    "    # 3. High-precision model predictions\n",
    "    high_precision_preds = high_precision_model.transform(test_data)\n",
    "\n",
    "    # 4. GBT model predictions (using optimized threshold)\n",
    "    gbt_preds = gbt_model.transform(test_data)\n",
    "    gbt_threshold_udf = create_threshold_udf(gbt_threshold)\n",
    "\n",
    "    # Fix: Directly use the probability column\n",
    "    # Extract the necessary columns from each prediction\n",
    "\n",
    "    # Original test_data columns to carry forward\n",
    "    predictions = test_data.select(\"id\", \"name\", \"artists\", \"label\")\n",
    "\n",
    "    # Fix: Do not use getItem() to access the probability vector\n",
    "    # Instead, use PySpark functions and UDFs to extract probability\n",
    "\n",
    "    # Create a UDF to extract the probability that a sample belongs to the positive class (class 1)\n",
    "    @udf(returnType=DoubleType())\n",
    "    def extract_probability_1(probability_vec):\n",
    "        # The second element in the probability vector corresponds to the positive class\n",
    "        if probability_vec is not None and len(probability_vec) > 1:\n",
    "            return float(probability_vec[1])\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the UDF to the high-recall model predictions\n",
    "    high_recall_with_prob = high_recall_preds.withColumn(\n",
    "        \"high_recall_prob\",\n",
    "        extract_probability_1(col(\"probability\"))\n",
    "    )\n",
    "\n",
    "    # Balanced model\n",
    "    balanced_with_prob = balanced_preds.withColumn(\n",
    "        \"balanced_prob\",\n",
    "        extract_probability_1(col(\"probability\"))\n",
    "    )\n",
    "\n",
    "    # High-precision model\n",
    "    high_precision_with_prob = high_precision_preds.withColumn(\n",
    "        \"high_precision_prob\",\n",
    "        extract_probability_1(col(\"probability\"))\n",
    "    )\n",
    "\n",
    "    # GBT model\n",
    "    gbt_with_prob = gbt_preds.withColumn(\n",
    "        \"gbt_prob\",\n",
    "        extract_probability_1(col(\"probability\"))\n",
    "    )\n",
    "\n",
    "    # Join each model’s predicted probabilities and decisions to the main predictions DataFrame\n",
    "    predictions = predictions.join(\n",
    "        high_recall_with_prob.select(\n",
    "            \"id\",\n",
    "            \"high_recall_prob\",\n",
    "            col(\"prediction\").alias(\"high_recall_pred\")\n",
    "        ),\n",
    "        \"id\"\n",
    "    )\n",
    "\n",
    "    predictions = predictions.join(\n",
    "        balanced_with_prob.select(\n",
    "            \"id\",\n",
    "            \"balanced_prob\",\n",
    "            col(\"prediction\").alias(\"balanced_pred\")\n",
    "        ),\n",
    "        \"id\"\n",
    "    )\n",
    "\n",
    "    predictions = predictions.join(\n",
    "        high_precision_with_prob.select(\n",
    "            \"id\",\n",
    "            \"high_precision_prob\",\n",
    "            col(\"prediction\").alias(\"high_precision_pred\")\n",
    "        ),\n",
    "        \"id\"\n",
    "    )\n",
    "\n",
    "    predictions = predictions.join(\n",
    "        gbt_with_prob.select(\n",
    "            \"id\",\n",
    "            \"gbt_prob\"\n",
    "        ),\n",
    "        \"id\"\n",
    "    )\n",
    "\n",
    "    # Apply the GBT threshold\n",
    "    predictions = predictions.withColumn(\n",
    "        \"gbt_pred\",\n",
    "        when(col(\"gbt_prob\") > gbt_threshold, 1.0).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Define a UDF for confidence level based on three model probabilities\n",
    "    @udf(returnType=StringType())\n",
    "    def get_confidence_level(prob_1, prob_2, prob_3):\n",
    "        \"\"\"\n",
    "        Determine the confidence level based on three model probabilities.\n",
    "        Uses a weighted average, giving higher weight to the high-recall model.\n",
    "        \"\"\"\n",
    "        weighted_prob = 0.5 * prob_1 + 0.3 * prob_2 + 0.2 * prob_3\n",
    "\n",
    "        if weighted_prob >= 0.7:\n",
    "            return \"Tier A (Highly Likely to Chart)\"\n",
    "        elif weighted_prob >= 0.5:\n",
    "            return \"Tier B (Moderately Likely to Chart)\"\n",
    "        elif weighted_prob >= 0.3:\n",
    "            return \"Tier C (Low Likelihood to Chart)\"\n",
    "        else:\n",
    "            return \"Tier D (Unlikely to Chart)\"\n",
    "\n",
    "    # Define a UDF for marketing recommendations based on confidence level\n",
    "    @udf(returnType=StringType())\n",
    "    def get_marketing_recommendation(confidence_level):\n",
    "        \"\"\"\n",
    "        Provide marketing recommendations based on the confidence level.\n",
    "        \"\"\"\n",
    "        if confidence_level == \"Tier A (Highly Likely to Chart)\":\n",
    "            return \"Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.\"\n",
    "        elif confidence_level == \"Tier B (Moderately Likely to Chart)\":\n",
    "            return \"Moderate Promotion: Social media promotion, partial platform recommendations, limited tour support.\"\n",
    "        elif confidence_level == \"Tier C (Low Likelihood to Chart)\":\n",
    "            return \"Low Promotion: Limited social media campaigns targeting specific audiences.\"\n",
    "        else:\n",
    "            return \"Basic Support: Standard release procedures and observation.\"\n",
    "\n",
    "    # Compute an ensemble prediction using a weighted vote approach\n",
    "    predictions = predictions.withColumn(\n",
    "        \"ensemble_vote\",\n",
    "        col(\"high_recall_pred\") * 0.4 +\n",
    "        col(\"balanced_pred\") * 0.3 +\n",
    "        col(\"high_precision_pred\") * 0.2 +\n",
    "        col(\"gbt_pred\") * 0.1\n",
    "    )\n",
    "\n",
    "    predictions = predictions.withColumn(\n",
    "        \"ensemble_pred\",\n",
    "        when(col(\"ensemble_vote\") >= 0.5, 1.0).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Add confidence level\n",
    "    predictions = predictions.withColumn(\n",
    "        \"confidence_level\",\n",
    "        get_confidence_level(\n",
    "            col(\"high_recall_prob\"),\n",
    "            col(\"balanced_prob\"),\n",
    "            col(\"high_precision_prob\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add marketing recommendation\n",
    "    predictions = predictions.withColumn(\n",
    "        \"marketing_recommendation\",\n",
    "        get_marketing_recommendation(col(\"confidence_level\"))\n",
    "    )\n",
    "\n",
    "    # Add business logic explanation\n",
    "    predictions = predictions.withColumn(\n",
    "        \"business_logic\",\n",
    "        expr(\n",
    "            \"CASE \"\n",
    "            \"WHEN confidence_level = 'Tier A (Highly Likely to Chart)' THEN 'Highest revenue potential, highest estimated return on investment' \"\n",
    "            \"WHEN confidence_level = 'Tier B (Moderately Likely to Chart)' THEN 'Moderate revenue potential, decent ROI with appropriate investment' \"\n",
    "            \"WHEN confidence_level = 'Tier C (Low Likelihood to Chart)' THEN 'Limited revenue potential, suitable for small-scale testing' \"\n",
    "            \"ELSE 'Lower revenue potential, minimal investment recommended' END\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a simplified reason for charting\n",
    "    predictions = predictions.withColumn(\n",
    "        \"ranking_reason\",\n",
    "        when(col(\"high_recall_prob\") > 0.7, \"Strong recommendation from the high-recall model\")\n",
    "        .when(col(\"balanced_prob\") > 0.7, \"Strong recommendation from the balanced model\")\n",
    "        .when(col(\"high_precision_prob\") > 0.7, \"Strong recommendation from the high-precision model\")\n",
    "        .when(col(\"high_recall_prob\") > 0.5, \"Multiple models collectively recommend\")\n",
    "        .otherwise(\"Evaluated based on multiple features\")\n",
    "    )\n",
    "\n",
    "    # Add a composite hit score\n",
    "    predictions = predictions.withColumn(\n",
    "        \"hit_score\",\n",
    "        col(\"high_recall_prob\") * 0.4 +\n",
    "        col(\"balanced_prob\") * 0.3 +\n",
    "        col(\"high_precision_prob\") * 0.2 +\n",
    "        col(\"gbt_prob\") * 0.1\n",
    "    )\n",
    "\n",
    "    # Add development guidance\n",
    "    predictions = predictions.withColumn(\n",
    "        \"development_guide\",\n",
    "        when(\n",
    "            col(\"confidence_level\").contains(\"Tier A\") | col(\"confidence_level\").contains(\"Tier B\"),\n",
    "            \"Already shows chart potential; maintain current style\"\n",
    "        )\n",
    "        .when(\n",
    "            col(\"confidence_level\").contains(\"Tier C\"),\n",
    "            \"Requires minor adjustments; consider features of other successful tracks\"\n",
    "        )\n",
    "        .otherwise(\"Requires major adjustments or a new direction\")\n",
    "    )\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Define the threshold adjustment function\n",
    "def create_threshold_udf(threshold):\n",
    "    @udf(returnType=DoubleType())\n",
    "    def apply_threshold(probability):\n",
    "        if probability is None:\n",
    "            return 0.0\n",
    "\n",
    "        # Check probability type and length\n",
    "        if isinstance(probability, list) and len(probability) > 1:\n",
    "            return 1.0 if float(probability[1]) > threshold else 0.0\n",
    "        return 0.0\n",
    "\n",
    "    return apply_threshold\n",
    "\n",
    "# Create the tiered prediction system\n",
    "print(\"Creating the tiered prediction system...\")\n",
    "tiered_predictions = create_tiered_predictions(\n",
    "    test_data,\n",
    "    rf_high_recall_model,\n",
    "    rf_balanced_model,\n",
    "    rf_high_precision_model,\n",
    "    gbt_model,\n",
    "    best_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de59754d-55fa-43e0-a6c5-8b4cc1e8ccde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 7. Results Analysis & Marketing Recommendations =====\n",
      "\n",
      "Confidence Level Distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:23:25 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:23:26 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:23:37 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "25/04/13 19:23:45 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "25/04/13 19:23:45 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "25/04/13 19:23:48 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier B (Moderately Likely to Chart): 5565 songs (9.99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier C (Low Likelihood to Chart): 9506 songs (17.06%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier D (Unlikely to Chart): 40412 songs (72.53%)\n",
      "\n",
      "Actual Hit Rate by Confidence Level:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:24:06 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:24:06 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:24:12 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "25/04/13 19:24:19 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:24:19 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:24:30 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier A (Highly Likely to Chart): Actual hit rate 26.58% (63/237)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:24:35 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:24:36 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:24:44 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "25/04/13 19:24:49 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:24:49 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:25:02 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier B (Moderately Likely to Chart): Actual hit rate 10.93% (608/5565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:25:06 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:25:07 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:25:13 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "25/04/13 19:25:17 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:25:17 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:25:25 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier C (Low Likelihood to Chart): Actual hit rate 4.70% (447/9506)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:25:30 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:25:30 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:25:36 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "25/04/13 19:25:40 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:25:40 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:25:48 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier D (Unlikely to Chart): Actual hit rate 0.81% (327/40412)\n",
      "\n",
      "Tier A (Highly Likely to Chart) Sample Songs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:25:54 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:25:54 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:25:54 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:26:09 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------------------------+------------------+-----------------------------------------------------------------------------------------------------+------------------------------------------------+-----------------------------------------------------+\n",
      "|name          |artists                                |hit_score         |marketing_recommendation                                                                             |ranking_reason                                  |development_guide                                    |\n",
      "+--------------+---------------------------------------+------------------+-----------------------------------------------------------------------------------------------------+------------------------------------------------+-----------------------------------------------------+\n",
      "|Juicy         |['Doja Cat', 'Tyga']                   |0.7919033608166763|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|X Ti          |['Sech']                               |0.7881499502645082|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|Matarse Solita|['Arcangel', 'Rauw Alejandro', 'Randy']|0.7873096427121077|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|Déjale Saber  |['Maluma']                             |0.7819536416371097|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|Go To Town    |['Doja Cat']                           |0.781497811456125 |Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "+--------------+---------------------------------------+------------------+-----------------------------------------------------------------------------------------------------+------------------------------------------------+-----------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Tier A (Highly Likely to Chart) but Actually Not Hits:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:26:14 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:26:14 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:26:14 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:26:24 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------------------+------------------------------------------------+\n",
      "|name             |artists            |hit_score         |ranking_reason                                  |\n",
      "+-----------------+-------------------+------------------+------------------------------------------------+\n",
      "|Wofür            |['Marie Reim']     |0.7041628287917348|Strong recommendation from the high-recall model|\n",
      "|Therapy          |['David Archuleta']|0.7086390388549487|Strong recommendation from the high-recall model|\n",
      "|Only One         |['Johnny Gill']    |0.7044248090877563|Strong recommendation from the high-recall model|\n",
      "|Stay All Night   |['ALMA']           |0.7055918727861941|Strong recommendation from the high-recall model|\n",
      "|No Money, No Love|['Etana']          |0.709037127603686 |Strong recommendation from the high-recall model|\n",
      "+-----------------+-------------------+------------------+------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Tier D (Unlikely to Chart) but Actually Hits:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:26:29 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:26:29 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:26:29 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:26:39 WARN DAGScheduler: Broadcasting large task binary with size 18.2 MiB\n",
      "[Stage 535:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+-------------------+------------------------------------+\n",
      "|name                       |artists             |hit_score          |ranking_reason                      |\n",
      "+---------------------------+--------------------+-------------------+------------------------------------+\n",
      "|Reason to Believe          |['Arch Enemy']      |0.22295077851085793|Evaluated based on multiple features|\n",
      "|Em Day Chang Phai Thuy Kieu|['Hoang Thuy Linh'] |0.2236708457915555 |Evaluated based on multiple features|\n",
      "|Algorhythm                 |['Childish Gambino']|0.19923464394175383|Evaluated based on multiple features|\n",
      "|Love Is The Main Thing     |['Fontaines D.C.']  |0.10185291784007908|Evaluated based on multiple features|\n",
      "|Sjung högre                |['Håkan Hellström'] |0.14091588373019265|Evaluated based on multiple features|\n",
      "+---------------------------+--------------------+-------------------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Analysis code after fixes\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Results Analysis & Marketing Recommendations\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n===== 7. Results Analysis & Marketing Recommendations =====\")\n",
    "\n",
    "# Analyze the distribution of confidence levels\n",
    "confidence_dist = tiered_predictions.groupBy(\"confidence_level\").count().orderBy(\"confidence_level\")\n",
    "print(\"\\nConfidence Level Distribution:\")\n",
    "\n",
    "confidence_counts = confidence_dist.collect()\n",
    "for row in confidence_counts:\n",
    "    total = tiered_predictions.count()\n",
    "    print(f\"{row['confidence_level']}: {row['count']} songs ({row['count'] / total * 100:.2f}%)\")\n",
    "\n",
    "# Analyze the actual hit rate for each confidence level\n",
    "print(\"\\nActual Hit Rate by Confidence Level:\")\n",
    "for level in [\n",
    "    \"Tier A (Highly Likely to Chart)\", \n",
    "    \"Tier B (Moderately Likely to Chart)\", \n",
    "    \"Tier C (Low Likelihood to Chart)\", \n",
    "    \"Tier D (Unlikely to Chart)\"\n",
    "]:\n",
    "    level_df = tiered_predictions.filter(col(\"confidence_level\") == level)\n",
    "    level_count = level_df.count()\n",
    "\n",
    "    if level_count > 0:\n",
    "        actual_hits = level_df.filter(col(\"label\") == 1.0).count()\n",
    "        hit_rate = actual_hits / level_count\n",
    "        print(f\"{level}: Actual hit rate {hit_rate * 100:.2f}% ({actual_hits}/{level_count})\")\n",
    "\n",
    "# Display sample songs predicted with high confidence\n",
    "print(\"\\nTier A (Highly Likely to Chart) Sample Songs:\")\n",
    "a_level_samples = (\n",
    "    tiered_predictions\n",
    "    .filter(col(\"confidence_level\") == \"Tier A (Highly Likely to Chart)\")\n",
    "    .select(\n",
    "        \"name\", \n",
    "        \"artists\", \n",
    "        \"hit_score\", \n",
    "        \"marketing_recommendation\", \n",
    "        \"ranking_reason\", \n",
    "        \"development_guide\"\n",
    "    )\n",
    "    .orderBy(col(\"hit_score\").desc())\n",
    ")\n",
    "a_level_samples.show(5, truncate=False)\n",
    "\n",
    "# Display Tier A songs that were actually not hits (false positives)\n",
    "print(\"\\nTier A (Highly Likely to Chart) but Actually Not Hits:\")\n",
    "false_positives = (\n",
    "    tiered_predictions\n",
    "    .filter(\n",
    "        (col(\"confidence_level\") == \"Tier A (Highly Likely to Chart)\") & \n",
    "        (col(\"label\") == 0.0)\n",
    "    )\n",
    "    .select(\"name\", \"artists\", \"hit_score\", \"ranking_reason\")\n",
    ")\n",
    "false_positives.show(5, truncate=False)\n",
    "\n",
    "# Display Tier D songs that were actually hits (false negatives)\n",
    "print(\"\\nTier D (Unlikely to Chart) but Actually Hits:\")\n",
    "false_negatives = (\n",
    "    tiered_predictions\n",
    "    .filter(\n",
    "        (col(\"confidence_level\") == \"Tier D (Unlikely to Chart)\") & \n",
    "        (col(\"label\") == 1.0)\n",
    "    )\n",
    "    .select(\"name\", \"artists\", \"hit_score\", \"ranking_reason\")\n",
    ")\n",
    "false_negatives.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb9a56cd-650b-4d92-9eca-86fa6f4501ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 8. Exporting Results for the Marketing Team =====\n",
      "\n",
      "Recommended List for the Marketing Team (Top 10):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:26:44 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:26:44 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:26:44 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:26:57 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------------------------+-------------------------------+------------------+-----------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------+-----------------------------------------------------+\n",
      "|name                  |artists                                |confidence_level               |hit_score         |marketing_recommendation                                                                             |business_logic                                                   |ranking_reason                                  |development_guide                                    |\n",
      "+----------------------+---------------------------------------+-------------------------------+------------------+-----------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------+-----------------------------------------------------+\n",
      "|Juicy                 |['Doja Cat', 'Tyga']                   |Tier A (Highly Likely to Chart)|0.7919033608166763|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Highest revenue potential, highest estimated return on investment|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|X Ti                  |['Sech']                               |Tier A (Highly Likely to Chart)|0.7881499502645082|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Highest revenue potential, highest estimated return on investment|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|Matarse Solita        |['Arcangel', 'Rauw Alejandro', 'Randy']|Tier A (Highly Likely to Chart)|0.7873096427121077|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Highest revenue potential, highest estimated return on investment|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|Déjale Saber          |['Maluma']                             |Tier A (Highly Likely to Chart)|0.7819536416371097|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Highest revenue potential, highest estimated return on investment|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|Go To Town            |['Doja Cat']                           |Tier A (Highly Likely to Chart)|0.781497811456125 |Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Highest revenue potential, highest estimated return on investment|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|So Much More Than This|['Grace VanderWaal']                   |Tier A (Highly Likely to Chart)|0.7780531435341205|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Highest revenue potential, highest estimated return on investment|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|Supuestamente         |['Ozuna', 'Anuel AA']                  |Tier A (Highly Likely to Chart)|0.7779865026114228|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Highest revenue potential, highest estimated return on investment|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|Tú Quieres            |['J Alvarez', 'El Micha']              |Tier A (Highly Likely to Chart)|0.7774659956885869|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Highest revenue potential, highest estimated return on investment|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|Sola Solita           |['La Fragua Band']                     |Tier A (Highly Likely to Chart)|0.7769215346017138|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Highest revenue potential, highest estimated return on investment|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "|Esa Boquita - Remix   |['J Alvarez', 'Zion & Lennox']         |Tier A (Highly Likely to Chart)|0.7678480114707422|Full Promotion: Major social media campaigns, platform recommendations, media coverage, artist tours.|Highest revenue potential, highest estimated return on investment|Strong recommendation from the high-recall model|Already shows chart potential; maintain current style|\n",
      "+----------------------+---------------------------------------+-------------------------------+------------------+-----------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------+-----------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:27:02 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:27:02 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:27:02 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:27:12 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "25/04/13 19:27:16 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "25/04/13 19:27:20 WARN DAGScheduler: Broadcasting large task binary with size 18.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Marketing recommendations saved to: /tmp/spotify_marketing_recommendations.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:27:24 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:27:25 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "25/04/13 19:27:25 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/13 19:27:35 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "25/04/13 19:27:38 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "25/04/13 19:27:42 WARN DAGScheduler: Broadcasting large task binary with size 18.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marketing recommendations saved to GCS: gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_marketing_recommendations.csv\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 8. Export Results for the Marketing Team\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n===== 8. Exporting Results for the Marketing Team =====\")\n",
    "\n",
    "# Create an output that is friendly for marketing usage – ensure all required columns exist\n",
    "available_columns = tiered_predictions.columns\n",
    "\n",
    "# Check if the required columns exist\n",
    "required_columns = [\n",
    "    \"name\", \n",
    "    \"artists\", \n",
    "    \"confidence_level\", \n",
    "    \"hit_score\",\n",
    "    \"marketing_recommendation\", \n",
    "    \"business_logic\",\n",
    "    \"ranking_reason\", \n",
    "    \"development_guide\"\n",
    "]\n",
    "\n",
    "# For each required column, add it as an empty column if it doesn't exist\n",
    "for column in required_columns:\n",
    "    if column not in available_columns:\n",
    "        tiered_predictions = tiered_predictions.withColumn(column, lit(None))\n",
    "\n",
    "# Select the necessary columns that are present\n",
    "marketing_columns = [col for col in required_columns if col in available_columns]\n",
    "marketing_output = tiered_predictions.select(marketing_columns)\n",
    "\n",
    "# Sort by hit_score in descending order\n",
    "marketing_output = marketing_output.orderBy(col(\"hit_score\").desc())\n",
    "\n",
    "# Display the first 10 rows as an example\n",
    "print(\"\\nRecommended List for the Marketing Team (Top 10):\")\n",
    "marketing_output.show(10, truncate=False)\n",
    "\n",
    "# Export results to CSV\n",
    "try:\n",
    "    # Convert to a pandas DataFrame for export\n",
    "    marketing_pandas = marketing_output.toPandas()\n",
    "    \n",
    "    # Save locally as a CSV file\n",
    "    csv_path = \"/tmp/spotify_marketing_recommendations.csv\"\n",
    "    marketing_pandas.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\nMarketing recommendations saved to: {csv_path}\")\n",
    "    \n",
    "    # Optionally save to Google Cloud Storage (GCS)\n",
    "    gcs_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_marketing_recommendations.csv\"\n",
    "    \n",
    "    # Use Spark to save to GCS\n",
    "    marketing_output.write.mode(\"overwrite\").option(\"header\", \"true\").csv(gcs_path)\n",
    "    print(f\"Marketing recommendations saved to GCS: {gcs_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during export: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d30f400-dc76-4ea2-aa62-f4ea8be46473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 9. Saving the Best Model =====\n",
      "Based on the F1 score, the best single model is: 1:3 balanced random forest model (balanced)\n",
      "F1: 0.1882, Precision: 0.1223, Recall: 0.4083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:27:47 WARN TaskSetManager: Stage 561 contains a task of very large size (3166 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved to: gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_rf_balanced_final_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:27:52 WARN TaskSetManager: Stage 568 contains a task of very large size (3104 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model rf_high_recall has been saved to: gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_rf_high_recall_final_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:27:57 WARN TaskSetManager: Stage 575 contains a task of very large size (3166 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model rf_balanced has been saved to: gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_rf_balanced_final_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 19:28:01 WARN TaskSetManager: Stage 582 contains a task of very large size (3129 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model rf_high_precision has been saved to: gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_rf_high_precision_final_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gbt has been saved to: gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_gbt_final_model\n",
      "Feature information saved to: /tmp/spotify_feature_info.json\n",
      "\n",
      "Spotify Hot Song Prediction System processing complete!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 9. Save the Best Model\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n===== 9. Saving the Best Model =====\")\n",
    "\n",
    "# Select the best model based on F1 score\n",
    "best_model_info = max(all_models, key=lambda x: x[\"f1\"])\n",
    "best_model_name = best_model_info[\"model\"]\n",
    "\n",
    "print(f\"Based on the F1 score, the best single model is: {best_model_name}\")\n",
    "print(f\"F1: {best_model_info['f1']:.4f}, Precision: {best_model_info['precision']:.4f}, Recall: {best_model_info['recall']:.4f}\")\n",
    "\n",
    "# Determine which model to save\n",
    "if \"1:2\" in best_model_name:\n",
    "    model_to_save = rf_high_recall_model\n",
    "    model_type = \"rf_high_recall\"\n",
    "elif \"1:3\" in best_model_name:\n",
    "    model_to_save = rf_balanced_model\n",
    "    model_type = \"rf_balanced\"\n",
    "elif \"1:5\" in best_model_name:\n",
    "    model_to_save = rf_high_precision_model\n",
    "    model_type = \"rf_high_precision\"\n",
    "else:\n",
    "    model_to_save = gbt_model\n",
    "    model_type = f\"gbt_threshold_{best_threshold}\"\n",
    "\n",
    "# Save the model\n",
    "model_path = f\"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_{model_type}_final_model\"\n",
    "model_to_save.write().overwrite().save(model_path)\n",
    "print(f\"The best model has been saved to: {model_path}\")\n",
    "\n",
    "# Save all models needed by the tiered prediction system\n",
    "models_to_save = {\n",
    "    \"rf_high_recall\": rf_high_recall_model,\n",
    "    \"rf_balanced\": rf_balanced_model,\n",
    "    \"rf_high_precision\": rf_high_precision_model,\n",
    "    \"gbt\": gbt_model\n",
    "}\n",
    "\n",
    "for name, model in models_to_save.items():\n",
    "    path = f\"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_{name}_final_model\"\n",
    "    model.write().overwrite().save(path)\n",
    "    print(f\"Model {name} has been saved to: {path}\")\n",
    "\n",
    "# Save feature information\n",
    "import json\n",
    "\n",
    "feature_info = {\n",
    "    \"audio_features\": audio_features,\n",
    "    \"new_features\": new_features,\n",
    "    \"feature_importances\": [(feat, float(imp)) for feat, imp in feature_importances],\n",
    "    \"best_threshold\": best_threshold\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Save feature information as a JSON file\n",
    "    with open(\"/tmp/spotify_feature_info.json\", \"w\") as f:\n",
    "        json.dump(feature_info, f, indent=2)\n",
    "    print(\"Feature information saved to: /tmp/spotify_feature_info.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving feature information: {e}\")\n",
    "\n",
    "print(\"\\nSpotify Hot Song Prediction System processing complete!\")\n",
    "\n",
    "# Close the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673123e6-abbf-4655-98af-8904528d9014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}