{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176cc785-40d8-4408-8254-fb3e0bb25fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/05 19:28:36 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/04/05 19:28:36 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/04/05 19:28:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/04/05 19:28:36 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spotify EDA\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a017587f-150a-4f6f-b966-5c957de726d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading deduplicated data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200290 tracks with 20 features\n",
      "Data schema:\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- album: string (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- duration_ms: double (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- af_danceability: double (nullable = true)\n",
      " |-- af_energy: double (nullable = true)\n",
      " |-- af_key: double (nullable = true)\n",
      " |-- af_loudness: double (nullable = true)\n",
      " |-- af_mode: double (nullable = true)\n",
      " |-- af_speechiness: double (nullable = true)\n",
      " |-- af_acousticness: double (nullable = true)\n",
      " |-- af_instrumentalness: double (nullable = true)\n",
      " |-- af_liveness: double (nullable = true)\n",
      " |-- af_valence: double (nullable = true)\n",
      " |-- af_tempo: double (nullable = true)\n",
      " |-- af_time_signature: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the deduplicated data\n",
    "deduplicated_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_deduplicated_data.csv\"\n",
    "print(\"Reading deduplicated data...\")\n",
    "spotify_df = spark.read.option(\"header\", \"true\").csv(deduplicated_path, inferSchema=True)\n",
    "\n",
    "# Verify data was loaded correctly\n",
    "print(f\"Loaded {spotify_df.count()} tracks with {len(spotify_df.columns)} features\")\n",
    "print(\"Data schema:\")\n",
    "spotify_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac7ad4-d931-4a79-8348-70f9db5aca02",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443d4b17-d545-4229-af57-afb3a352d6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== EXPLORATORY DATA ANALYSIS ==========\n",
      "\n",
      "Number of tracks: 200290\n",
      "Number of features: 20\n",
      "\n",
      "Basic statistics for numerical features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 19:29:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------------------+--------------------+--------------------+----------+------------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+------------------+------------------+\n",
      "|summary|   title|                 artist|              region|            track_id|     album|        popularity|      duration_ms|   af_danceability|         af_energy|           af_key|       af_loudness|            af_mode|     af_speechiness|    af_acousticness| af_instrumentalness|        af_liveness|         af_valence|          af_tempo| af_time_signature|\n",
      "+-------+--------+-----------------------+--------------------+--------------------+----------+------------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+------------------+------------------+\n",
      "|  count|  200289|                 200289|              200290|              200289|    199965|            197941|           199860|            197943|            199863|           200281|            200282|             200283|             200285|             200286|              200286|             200287|             200286|            200286|            200287|\n",
      "|   mean|Infinity|      6441.921571428572|                NULL|  30974.328719723184|  Infinity|21.244729490100585|211307.7197838487|1.6697363023698677|3.8640905804946453| 6.84227486781072|-7.191066321486676|0.48519611250081635|0.11451267443892436|0.27939941491342146|0.048110371120747834|0.18553537159785502|0.49549315318638876|120.05384514844788| 5.048558846305558|\n",
      "| stddev|     NaN|     27126.093732034544|                NULL|   92383.74633109421|       NaN| 22.58209846610518| 87063.7747522134| 453.2497270052327|1089.8623958171083|703.1875944976905|3.6316642706944084| 1.0053834741897836|0.28615881550226774| 0.2765066748764119|  0.1759082615354126| 3.6691875146239696|0.23377776283086685|31.277112057774595|11.700863674614107|\n",
      "|    min|       !|                    !!!|          2017-11-01|000RW47rhEkSqjgTr...|         !|               0.0|              0.0|               0.0|               0.0|              0.0|             -60.0|             -14.58|            -10.169|            -20.527|              -5.092|             -5.676|             -3.784|               0.0|               0.0|\n",
      "|    max|      ðŸª|ï¼­, å…©åƒ 2Ã˜Ã˜Ã˜, Redco...|https://open.spot...|             viral50|ï½¢untitledï½£|            2912.0|        9318296.0|          201655.0|          443404.0|         314697.0|              11.0|               10.0|                2.0|                7.0|                10.0|             1641.0|              0.999|           238.431|           230.094|\n",
      "+-------+--------+-----------------------+--------------------+--------------------+----------+------------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "print(\"\\n========== EXPLORATORY DATA ANALYSIS ==========\\n\")\n",
    "\n",
    "# Basic dataset info\n",
    "print(f\"Number of tracks: {spotify_df.count()}\")\n",
    "print(f\"Number of features: {len(spotify_df.columns)}\")\n",
    "\n",
    "# Compute basic statistics for numerical features\n",
    "print(\"\\nBasic statistics for numerical features:\")\n",
    "spotify_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9291cd97-cf72-4fa1-a5f2-578e81fee16f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of explicit content:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|explicit| count|\n",
      "+--------+------+\n",
      "|   false|148553|\n",
      "|    true| 49387|\n",
      "|    NULL|  2350|\n",
      "+--------+------+\n",
      "\n",
      "\n",
      "Identified audio features: ['af_danceability', 'af_energy', 'af_key', 'af_loudness', 'af_mode', 'af_speechiness', 'af_acousticness', 'af_instrumentalness', 'af_liveness', 'af_valence', 'af_tempo', 'af_time_signature']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show distribution of explicit flag if present\n",
    "if \"explicit\" in spotify_df.columns:\n",
    "    print(\"\\nDistribution of explicit content:\")\n",
    "    spotify_df.groupBy(\"explicit\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "# Identify audio features (columns starting with 'af_')\n",
    "audio_features = [col for col in spotify_df.columns if col.startswith('af_')]\n",
    "print(f\"\\nIdentified audio features: {audio_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a64baec-8362-4159-b757-eb1764503045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing correlation matrix for audio features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found null values in these features: {'af_danceability': 2347, 'af_energy': 427, 'af_key': 9, 'af_loudness': 8, 'af_mode': 7, 'af_speechiness': 5, 'af_acousticness': 4, 'af_instrumentalness': 4, 'af_liveness': 3, 'af_valence': 4, 'af_tempo': 4, 'af_time_signature': 3}\n",
      "Dropping rows with null values in audio features for correlation calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation Matrix:\n",
      "Features: af_danceability, af_energy, af_key, af_loudness, af_mode, af_speechiness, af_acousticness, af_instrumentalness, af_liveness, af_valence, af_tempo, af_time_signature\n",
      "af_danceability: [1.0, 0.15, 0.01, 0.17, -0.09, 0.23, -0.22, -0.13, -0.09, 0.38, -0.09, 0.15]\n",
      "af_energy: [0.15, 1.0, 0.03, 0.74, -0.06, 0.06, -0.61, -0.12, 0.17, 0.37, 0.12, 0.13]\n",
      "af_key: [0.01, 0.03, 1.0, 0.01, -0.16, 0.02, -0.01, -0.0, 0.0, 0.03, -0.0, 0.0]\n",
      "af_loudness: [0.17, 0.74, 0.01, 1.0, -0.03, -0.01, -0.46, -0.33, 0.09, 0.27, 0.09, 0.11]\n",
      "af_mode: [-0.09, -0.06, -0.16, -0.03, 1.0, -0.07, 0.05, -0.0, 0.01, -0.03, 0.01, -0.03]\n",
      "af_speechiness: [0.23, 0.06, 0.02, -0.01, -0.07, 1.0, -0.06, -0.12, 0.05, 0.07, 0.04, 0.04]\n",
      "af_acousticness: [-0.22, -0.61, -0.01, -0.46, 0.05, -0.06, 1.0, 0.08, -0.07, -0.16, -0.1, -0.13]\n",
      "af_instrumentalness: [-0.13, -0.12, -0.0, -0.33, -0.0, -0.12, 0.08, 1.0, -0.03, -0.15, -0.01, -0.05]\n",
      "af_liveness: [-0.09, 0.17, 0.0, 0.09, 0.01, 0.05, -0.07, -0.03, 1.0, 0.05, 0.02, -0.0]\n",
      "af_valence: [0.38, 0.37, 0.03, 0.27, -0.03, 0.07, -0.16, -0.15, 0.05, 1.0, 0.05, 0.07]\n",
      "af_tempo: [-0.09, 0.12, -0.0, 0.09, 0.01, 0.04, -0.1, -0.01, 0.02, 0.05, 1.0, -0.01]\n",
      "af_time_signature: [0.15, 0.13, 0.0, 0.11, -0.03, 0.04, -0.13, -0.05, -0.0, 0.07, -0.01, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute correlation matrix for audio features\n",
    "if len(audio_features) > 0:\n",
    "    print(\"\\nComputing correlation matrix for audio features...\")\n",
    "    # Create feature vector\n",
    "    assembler = VectorAssembler(inputCols=audio_features, outputCol=\"features\")\n",
    "    \n",
    "    # Check for null values in audio features\n",
    "    null_counts = {}\n",
    "    for feature in audio_features:\n",
    "        null_count = spotify_df.filter(col(feature).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            null_counts[feature] = null_count\n",
    "    \n",
    "    if null_counts:\n",
    "        print(f\"Warning: Found null values in these features: {null_counts}\")\n",
    "        print(\"Dropping rows with null values in audio features for correlation calculation\")\n",
    "        df_for_corr = spotify_df.dropna(subset=audio_features)\n",
    "    else:\n",
    "        df_for_corr = spotify_df\n",
    "    \n",
    "    feature_vector = assembler.transform(df_for_corr.select(audio_features))\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    correlation_matrix = Correlation.corr(feature_vector, \"features\").collect()[0][0]\n",
    "    correlation_matrix_np = correlation_matrix.toArray()\n",
    "    \n",
    "    # Print correlation matrix with feature names\n",
    "    print(\"\\nCorrelation Matrix:\")\n",
    "    print(\"Features: \" + \", \".join(audio_features))\n",
    "    for i, row in enumerate(correlation_matrix_np):\n",
    "        print(f\"{audio_features[i]}: {[round(x, 2) for x in row]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8461748e-b3fc-41cb-957c-2601822e9a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 tracks by popularity:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+\n",
      "|           title|              artist|popularity|\n",
      "+----------------+--------------------+----------+\n",
      "| Versace - Remix|\"El Mayor Clasico...|    2912.0|\n",
      "|    Cruel Summer|        Taylor Swift|      96.0|\n",
      "|       Unwritten| Natasha Bedingfield|      92.0|\n",
      "|         Starboy|The Weeknd, Daft ...|      91.0|\n",
      "|           Lover|        Taylor Swift|      90.0|\n",
      "|    Another Love|           Tom Odell|      90.0|\n",
      "| Blinding Lights|          The Weeknd|      90.0|\n",
      "|          Yellow|            Coldplay|      90.0|\n",
      "|The Night We Met|          Lord Huron|      90.0|\n",
      "|    Pink + White|         Frank Ocean|      89.0|\n",
      "+----------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Distribution of audio features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af_danceability:\n",
      "  25th percentile: 0.547\n",
      "  Median: 0.666\n",
      "  75th percentile: 0.763\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af_energy:\n",
      "  25th percentile: 0.517\n",
      "  Median: 0.652\n",
      "  75th percentile: 0.776\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af_key:\n",
      "  25th percentile: 2.0\n",
      "  Median: 5.0\n",
      "  75th percentile: 8.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af_loudness:\n",
      "  25th percentile: -8.644\n",
      "  Median: -6.75\n",
      "  75th percentile: -5.21\n",
      "\n",
      "af_mode:\n",
      "  25th percentile: 0.0\n",
      "  Median: 1.0\n",
      "  75th percentile: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af_speechiness:\n",
      "  25th percentile: 0.0393\n",
      "  Median: 0.0635\n",
      "  75th percentile: 0.158\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af_acousticness:\n",
      "  25th percentile: 0.0476\n",
      "  Median: 0.179\n",
      "  75th percentile: 0.44\n",
      "\n",
      "af_instrumentalness:\n",
      "  25th percentile: 0.0\n",
      "  Median: 1.55e-06\n",
      "  75th percentile: 0.000302\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af_liveness:\n",
      "  25th percentile: 0.0954\n",
      "  Median: 0.121\n",
      "  75th percentile: 0.21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af_valence:\n",
      "  25th percentile: 0.316\n",
      "  Median: 0.488\n",
      "  75th percentile: 0.676\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af_tempo:\n",
      "  25th percentile: 97.595\n",
      "  Median: 120.015\n",
      "  75th percentile: 139.946\n",
      "\n",
      "af_time_signature:\n",
      "  25th percentile: 4.0\n",
      "  Median: 4.0\n",
      "  75th percentile: 4.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find top 10 most popular tracks if popularity column exists\n",
    "if \"popularity\" in spotify_df.columns:\n",
    "    print(\"\\nTop 10 tracks by popularity:\")\n",
    "    spotify_df.orderBy(\"popularity\", ascending=False).select(\"title\", \"artist\", \"popularity\").show(10)\n",
    "\n",
    "# Calculate distribution of audio features\n",
    "print(\"\\nDistribution of audio features:\")\n",
    "for feature in audio_features:\n",
    "    try:\n",
    "        quartiles = spotify_df.approxQuantile(feature, [0.25, 0.5, 0.75], 0.01)\n",
    "        print(f\"{feature}:\")\n",
    "        print(f\"  25th percentile: {quartiles[0]}\")\n",
    "        print(f\"  Median: {quartiles[1]}\")\n",
    "        print(f\"  75th percentile: {quartiles[2]}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate quartiles for {feature}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b5e33f-e493-4e42-9c05-39ede71253c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of dataset:\n",
      "+--------------------+------------+---------+--------------------+--------------------+----------+-----------+--------+---------------+---------+------+-----------+-------+--------------+---------------+-------------------+-----------+----------+--------+-----------------+\n",
      "|               title|      artist|   region|            track_id|               album|popularity|duration_ms|explicit|af_danceability|af_energy|af_key|af_loudness|af_mode|af_speechiness|af_acousticness|af_instrumentalness|af_liveness|af_valence|af_tempo|af_time_signature|\n",
      "+--------------------+------------+---------+--------------------+--------------------+----------+-----------+--------+---------------+---------+------+-----------+-------+--------------+---------------+-------------------+-----------+----------+--------+-----------------+\n",
      "|Still Got Time (f...|        ZAYN|Australia|000xQL6tZNLJzIrtI...|Still Got Time (f...|      56.0|   188490.0|   false|          0.748|    0.627|   7.0|     -6.029|    1.0|        0.0639|          0.131|                0.0|     0.0852|     0.524| 120.963|              4.0|\n",
      "|Eu, VocÃª, O Mar e...|Luan Santana|   Brazil|000xYdQfIZ4pDmBGz...|                1977|      66.0|   187118.0|   false|          0.509|    0.803|   0.0|     -6.743|    1.0|          0.04|          0.684|            5.39E-4|      0.463|     0.651| 166.018|              4.0|\n",
      "|           Zun Da Da|        Zion| Honduras|007ogFejDqJKzEXDU...|  The Perfect Melody|       0.0|   303680.0|   false|          0.771|    0.769|   9.0|      -7.85|    0.0|         0.105|          0.266|            5.42E-5|     0.0804|     0.598|  88.001|              4.0|\n",
      "|                 Top|    YNY Sebi|  Romania|008A8MMPCshycPlNe...|                 Top|      25.0|   161926.0|    true|          0.803|     0.56|   2.0|     -8.433|    1.0|         0.103|         0.0484|             0.0256|      0.096|     0.379|  98.015|              4.0|\n",
      "|Ho Guardato Un'Altra|       Mecna|    Italy|00C3ZQ1QnyhVhGKZg...|Mentre Nessuno Gu...|      32.0|   187124.0|   false|          0.682|    0.741|   0.0|      -6.31|    1.0|        0.0421|        0.00122|            7.66E-5|      0.266|      0.15| 120.002|              4.0|\n",
      "+--------------------+------------+---------+--------------------+--------------------+----------+-----------+--------+---------------+---------+------+-----------+-------+--------------+---------------+-------------------+-----------+----------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print sample data\n",
    "print(\"\\nSample of dataset:\")\n",
    "spotify_df.show(5)\n",
    "\n",
    "# Stop SparkSession to release resources\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bfbef-e176-41a5-8cf9-6054cc8609a6",
   "metadata": {},
   "source": [
    "# EDA Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74acded4-951e-4d3e-8b28-968c6de9b26d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 19:54:55 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/04/05 19:54:55 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/04/05 19:54:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/04/05 19:54:55 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading deduplicated data from GCP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200290 tracks with 20 features\n",
      "Identified audio features: ['af_danceability', 'af_energy', 'af_key', 'af_loudness', 'af_mode', 'af_speechiness', 'af_acousticness', 'af_instrumentalness', 'af_liveness', 'af_valence', 'af_tempo', 'af_time_signature']\n",
      "Converting to pandas for visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200290 records to pandas DataFrame\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spotify Visualization\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the deduplicated data\n",
    "deduplicated_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_deduplicated_data.csv\"\n",
    "print(\"Reading deduplicated data from GCP...\")\n",
    "spotify_df = spark.read.option(\"header\", \"true\").csv(deduplicated_path, inferSchema=True)\n",
    "\n",
    "# Verify data was loaded correctly\n",
    "print(f\"Loaded {spotify_df.count()} tracks with {len(spotify_df.columns)} features\")\n",
    "\n",
    "# Identify audio features (columns starting with 'af_')\n",
    "audio_features = [col for col in spotify_df.columns if col.startswith('af_')]\n",
    "print(f\"Identified audio features: {audio_features}\")\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "print(\"Converting to pandas for visualization...\")\n",
    "pdf = spotify_df.toPandas()\n",
    "print(f\"Converted {len(pdf)} records to pandas DataFrame\")\n",
    "\n",
    "# Set up plotting environment\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7a7f6-3d78-474c-9f3f-2972f4de8e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting audio feature distributions...\n"
     ]
    }
   ],
   "source": [
    "# 1. Distribution of audio features\n",
    "print(\"Plotting audio feature distributions...\")\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(audio_features):\n",
    "    if i < len(axes):\n",
    "        sns.histplot(pdf[feature].dropna(), kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/audio_feature_distributions.png')\n",
    "print(\"Saved audio feature distributions to /tmp/audio_feature_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a0067-436a-4cad-911c-a294bf7f3fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Correlation heatmap\n",
    "print(\"Creating correlation heatmap...\")\n",
    "plt.figure(figsize=(14, 12))\n",
    "corr_matrix = pdf[audio_features].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', mask=mask, \n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .7})\n",
    "plt.title('Correlation between Audio Features', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/correlation_heatmap.png')\n",
    "print(\"Saved correlation heatmap to /tmp/correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a2a587-d5f3-4bdf-8a1c-78f33876abe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Popularity vs audio features (if popularity column exists)\n",
    "if \"popularity\" in pdf.columns:\n",
    "    print(\"Plotting popularity vs audio features...\")\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(audio_features):\n",
    "        if i < len(axes):\n",
    "            axes[i].scatter(pdf[feature], pdf['popularity'], alpha=0.5)\n",
    "            axes[i].set_title(f'{feature} vs Popularity')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('Popularity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/popularity_vs_features.png')\n",
    "    print(\"Saved popularity vs features plots to /tmp/popularity_vs_features.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9677301-935f-4f0b-9254-8e80c4ec5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Boxplot of audio features by explicit content (if explicit column exists)\n",
    "if \"explicit\" in pdf.columns:\n",
    "    print(\"Creating boxplots by explicit content...\")\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(audio_features):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(x='explicit', y=feature, data=pdf, ax=axes[i])\n",
    "            axes[i].set_title(f'{feature} by Explicit Content')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/features_by_explicit.png')\n",
    "    print(\"Saved features by explicit content boxplots to /tmp/features_by_explicit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da57877-d485-408e-b209-0b3f121157f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. PCA visualization\n",
    "print(\"Performing PCA analysis...\")\n",
    "# Prepare data for PCA\n",
    "features_for_pca = pdf[audio_features].dropna()\n",
    "\n",
    "if len(features_for_pca) > 0:\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features_for_pca)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Create a DataFrame with the principal components\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "    \n",
    "    # Add track information\n",
    "    pca_df['title'] = pdf.loc[features_for_pca.index, 'title'].values\n",
    "    pca_df['artist'] = pdf.loc[features_for_pca.index, 'artist'].values\n",
    "    if 'popularity' in pdf.columns:\n",
    "        pca_df['popularity'] = pdf.loc[features_for_pca.index, 'popularity'].values\n",
    "    \n",
    "    # Plot PCA\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], \n",
    "                         c=pca_df['popularity'] if 'popularity' in pca_df.columns else None, \n",
    "                         alpha=0.5, cmap='viridis')\n",
    "    plt.title('PCA of Audio Features', fontsize=18)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "    \n",
    "    if 'popularity' in pca_df.columns:\n",
    "        plt.colorbar(scatter, label='Popularity')\n",
    "    \n",
    "    plt.savefig('/tmp/pca_visualization.png')\n",
    "    print(\"Saved PCA visualization to /tmp/pca_visualization.png\")\n",
    "    print(f\"PCA explained variance: {pca.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9a3e1-8718-4dea-a66e-5e94c058d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Top artists by track count\n",
    "print(\"Analyzing top artists...\")\n",
    "top_artists = pdf['artist'].value_counts().head(20)\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.barplot(x=top_artists.values, y=top_artists.index)\n",
    "plt.title('Top 20 Artists by Number of Tracks', fontsize=18)\n",
    "plt.xlabel('Number of Tracks')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/top_artists.png')\n",
    "print(\"Saved top artists chart to /tmp/top_artists.png\")\n",
    "\n",
    "print(\"All visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb85baf3-a122-4120-aaab-8579654ca518",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aeee35-17ff-42a9-abf8-2247c989f21a",
   "metadata": {},
   "source": [
    "# data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071fb8f8-d67a-4661-afc3-edcf1e0da7af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/05 21:17:17 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/04/05 21:17:17 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/04/05 21:17:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/04/05 21:17:18 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spotify Classification Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854132f6-5763-4e14-88bc-999b9f12431d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LOADING DATASETS =====\n",
      "Reading deduplicated data from GCP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded deduplicated dataset with 200290 rows and 20 columns\n",
      "Sample of deduplicated data:\n",
      "+--------------------+------------+---------+--------------------+--------------------+----------+-----------+--------+---------------+---------+------+-----------+-------+--------------+---------------+-------------------+-----------+----------+--------+-----------------+\n",
      "|               title|      artist|   region|            track_id|               album|popularity|duration_ms|explicit|af_danceability|af_energy|af_key|af_loudness|af_mode|af_speechiness|af_acousticness|af_instrumentalness|af_liveness|af_valence|af_tempo|af_time_signature|\n",
      "+--------------------+------------+---------+--------------------+--------------------+----------+-----------+--------+---------------+---------+------+-----------+-------+--------------+---------------+-------------------+-----------+----------+--------+-----------------+\n",
      "|Still Got Time (f...|        ZAYN|Australia|000xQL6tZNLJzIrtI...|Still Got Time (f...|      56.0|   188490.0|   false|          0.748|    0.627|   7.0|     -6.029|    1.0|        0.0639|          0.131|                0.0|     0.0852|     0.524| 120.963|              4.0|\n",
      "|Eu, VocÃª, O Mar e...|Luan Santana|   Brazil|000xYdQfIZ4pDmBGz...|                1977|      66.0|   187118.0|   false|          0.509|    0.803|   0.0|     -6.743|    1.0|          0.04|          0.684|            5.39E-4|      0.463|     0.651| 166.018|              4.0|\n",
      "|           Zun Da Da|        Zion| Honduras|007ogFejDqJKzEXDU...|  The Perfect Melody|       0.0|   303680.0|   false|          0.771|    0.769|   9.0|      -7.85|    0.0|         0.105|          0.266|            5.42E-5|     0.0804|     0.598|  88.001|              4.0|\n",
      "|                 Top|    YNY Sebi|  Romania|008A8MMPCshycPlNe...|                 Top|      25.0|   161926.0|    true|          0.803|     0.56|   2.0|     -8.433|    1.0|         0.103|         0.0484|             0.0256|      0.096|     0.379|  98.015|              4.0|\n",
      "|Ho Guardato Un'Altra|       Mecna|    Italy|00C3ZQ1QnyhVhGKZg...|Mentre Nessuno Gu...|      32.0|   187124.0|   false|          0.682|    0.741|   0.0|      -6.31|    1.0|        0.0421|        0.00122|            7.66E-5|      0.266|      0.15| 120.002|              4.0|\n",
      "+--------------------+------------+---------+--------------------+--------------------+----------+-----------+--------+---------------+---------+------+-----------+-------+--------------+---------------+-------------------+-----------+----------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Reading tracks features data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tracks features dataset with 1204025 rows and 24 columns\n",
      "Sample of tracks features data:\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----------+--------+------------+------------------+---+-------------------+----+-----------+------------+----------------+-------------------+-------+-----------------+-----------+--------------+----+------------+\n",
      "|                  id|                name|               album|            album_id|             artists|          artist_ids|track_number|disc_number|explicit|danceability|            energy|key|           loudness|mode|speechiness|acousticness|instrumentalness|           liveness|valence|            tempo|duration_ms|time_signature|year|release_date|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----------+--------+------------+------------------+---+-------------------+----+-----------+------------+----------------+-------------------+-------+-----------------+-----------+--------------+----+------------+\n",
      "|7lmeHLHBe4nmXzuXc...|             Testify|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           1|          1|   False|        0.47|             0.978|  7|             -5.399|   1|     0.0727|      0.0261|        1.09e-05|0.35600000000000004|  0.503|          117.906|     210133|           4.0|1999|  1999-11-02|\n",
      "|1wsRitfRRtWyEapl0...|     Guerrilla Radio|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           2|          1|    True|       0.599|0.9570000000000001| 11| -5.763999999999999|   1|      0.188|      0.0129|        7.06e-05|              0.155|  0.489|           103.68|     206200|           4.0|1999|  1999-11-02|\n",
      "|1hR0fIFK2qRG3f3RF...|    Calm Like a Bomb|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           3|          1|   False|       0.315|              0.97|  7|-5.4239999999999995|   1|      0.483|      0.0234|        2.03e-06|              0.122|   0.37|          149.749|     298893|           4.0|1999|  1999-11-02|\n",
      "|2lbASgTSoDO7MTuLA...|           Mic Check|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           4|          1|    True|        0.44|0.9670000000000001| 11|              -5.83|   0|      0.237|       0.163|        3.64e-06|              0.121|  0.574|96.75200000000001|     213640|           4.0|1999|  1999-11-02|\n",
      "|1MQTmpYOZ6fcMQc56...|Sleep Now In the ...|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           5|          1|   False|       0.426|             0.929|  2|             -6.729|   1|     0.0701|     0.00162|           0.105|             0.0789|  0.539|          127.059|     205600|           4.0|1999|  1999-11-02|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----------+--------+------------+------------------+---+-------------------+----+-----------+------------+----------------+-------------------+-------+-----------------+-----------+--------------+----+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Schema of deduplicated dataset:\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- album: string (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- duration_ms: double (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- af_danceability: double (nullable = true)\n",
      " |-- af_energy: double (nullable = true)\n",
      " |-- af_key: double (nullable = true)\n",
      " |-- af_loudness: double (nullable = true)\n",
      " |-- af_mode: double (nullable = true)\n",
      " |-- af_speechiness: double (nullable = true)\n",
      " |-- af_acousticness: double (nullable = true)\n",
      " |-- af_instrumentalness: double (nullable = true)\n",
      " |-- af_liveness: double (nullable = true)\n",
      " |-- af_valence: double (nullable = true)\n",
      " |-- af_tempo: double (nullable = true)\n",
      " |-- af_time_signature: double (nullable = true)\n",
      "\n",
      "\n",
      "Schema of tracks features dataset:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- album: string (nullable = true)\n",
      " |-- album_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- artist_ids: string (nullable = true)\n",
      " |-- track_number: string (nullable = true)\n",
      " |-- disc_number: string (nullable = true)\n",
      " |-- explicit: string (nullable = true)\n",
      " |-- danceability: string (nullable = true)\n",
      " |-- energy: string (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- loudness: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- speechiness: string (nullable = true)\n",
      " |-- acousticness: string (nullable = true)\n",
      " |-- instrumentalness: string (nullable = true)\n",
      " |-- liveness: string (nullable = true)\n",
      " |-- valence: string (nullable = true)\n",
      " |-- tempo: string (nullable = true)\n",
      " |-- duration_ms: string (nullable = true)\n",
      " |-- time_signature: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load datasets\n",
    "print(\"\\n===== LOADING DATASETS =====\")\n",
    "\n",
    "# Load the deduplicated data from GCS\n",
    "deduplicated_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_deduplicated_data.csv\"\n",
    "print(\"Reading deduplicated data from GCP...\")\n",
    "spotify_df = spark.read.option(\"header\", \"true\").csv(deduplicated_path, inferSchema=True)\n",
    "\n",
    "# Check if the data was loaded successfully\n",
    "print(f\"Loaded deduplicated dataset with {spotify_df.count()} rows and {len(spotify_df.columns)} columns\")\n",
    "print(\"Sample of deduplicated data:\")\n",
    "spotify_df.show(5)\n",
    "\n",
    "# Load the tracks features dataset\n",
    "tracks_features_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/tracks_features.csv\"\n",
    "print(\"\\nReading tracks features data...\")\n",
    "tracks_features_df = spark.read.option(\"header\", \"true\").csv(tracks_features_path, inferSchema=True)\n",
    "\n",
    "# Check if the data was loaded successfully\n",
    "print(f\"Loaded tracks features dataset with {tracks_features_df.count()} rows and {len(tracks_features_df.columns)} columns\")\n",
    "print(\"Sample of tracks features data:\")\n",
    "tracks_features_df.show(5)\n",
    "\n",
    "# Print the schema of both datasets to understand their structure\n",
    "print(\"\\nSchema of deduplicated dataset:\")\n",
    "spotify_df.printSchema()\n",
    "\n",
    "print(\"\\nSchema of tracks features dataset:\")\n",
    "tracks_features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c5ba671-abc7-4b49-8fda-a7cd0bf88c65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FILTERING SONGS BY YEAR =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs from 2015-2021: 336181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 2: Filter songs from 2015-2021 in the tracks features dataset\n",
    "print(\"\\n===== FILTERING SONGS BY YEAR =====\")\n",
    "\n",
    "# Convert year column to integer for comparison\n",
    "tracks_features_df = tracks_features_df.withColumn(\"year\", col(\"year\").cast(\"integer\"))\n",
    "\n",
    "# Filter songs between 2015 and 2021\n",
    "filtered_tracks_df = tracks_features_df.filter((col(\"year\") >= 2015) & (col(\"year\") <= 2021))\n",
    "\n",
    "# Count the number of songs after filtering\n",
    "filtered_count = filtered_tracks_df.count()\n",
    "print(f\"Songs from 2015-2021: {filtered_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b75683d6-68fd-4d8a-b3cb-f33a37c09813",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EXTRACTING UNIQUE ARTISTS FROM RANK DATASET =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 86341 unique artists from the rank dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, trim, collect_set\n",
    "\n",
    "# Step 3: Extract unique artists from the rank dataset\n",
    "print(\"\\n===== EXTRACTING UNIQUE ARTISTS FROM RANK DATASET =====\")\n",
    "\n",
    "# Collect all unique artists from the artist column\n",
    "# The artists column might contain multiple artists separated by commas\n",
    "# We'll collect them all as a single set\n",
    "unique_artists = spotify_df.select(\"artist\").distinct()\n",
    "\n",
    "# Count the number of unique artists\n",
    "artist_count = unique_artists.count()\n",
    "print(f\"Extracted {artist_count} unique artists from the rank dataset\")\n",
    "\n",
    "# Alternatively, we can persist this information without collecting\n",
    "unique_artists_df = unique_artists.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51232e52-d003-4b18-aef7-3f8cdbbd91bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FILTERING SONGS BY RANKED ARTISTS (SUBSTRING MATCHING) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs by ranked artists (substring matching): 278731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, lit\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Step 4 (Re-revised): Filter songs by ranked artists using substring matching\n",
    "print(\"\\n===== FILTERING SONGS BY RANKED ARTISTS (SUBSTRING MATCHING) =====\")\n",
    "\n",
    "# Get unique artists from the rank dataset\n",
    "unique_artists_df = spotify_df.select(\"artist\").distinct().filter(col(\"artist\").isNotNull())\n",
    "\n",
    "# Collect artist names as a list\n",
    "ranked_artists_list = [row.artist for row in unique_artists_df.collect()]\n",
    "broadcast_artists = spark.sparkContext.broadcast(ranked_artists_list)\n",
    "\n",
    "# Function to check if any ranked artist appears within the song's artists string\n",
    "def artist_substring_match(artists_str, artist_list_broadcast):\n",
    "    if artists_str is None:\n",
    "        return False\n",
    "    \n",
    "    for artist in artist_list_broadcast.value:\n",
    "        if artist is not None and artist in artists_str:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Register as UDF\n",
    "artist_match_udf = udf(\n",
    "    lambda artists_str: artist_substring_match(artists_str, broadcast_artists),\n",
    "    BooleanType()\n",
    ")\n",
    "\n",
    "# Apply the filter using substring matching (as in the original pandas code)\n",
    "filtered_songs_df = filtered_tracks_df.filter(\n",
    "    col(\"artists\").isNotNull() & \n",
    "    artist_match_udf(col(\"artists\"))\n",
    ")\n",
    "\n",
    "# Count the number of filtered songs\n",
    "filtered_songs_count = filtered_songs_df.count()\n",
    "print(f\"Songs by ranked artists (substring matching): {filtered_songs_count}\")\n",
    "\n",
    "# Cache the filtered data for subsequent operations\n",
    "filtered_songs_df = filtered_songs_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfd93c23-5b06-4a39-8058-908d841363c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CREATING BINARY LABELS =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs that appeared in rankings: 7510\n",
      "Songs that did not appear in rankings: 271221\n",
      "Percentage of songs in rankings: 2.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# Step 5: Create labels - check if songs are in the rank dataset\n",
    "print(\"\\n===== CREATING BINARY LABELS =====\")\n",
    "\n",
    "# Extract track IDs from the rank dataset (spotify_df)\n",
    "ranked_track_ids_df = spotify_df.select(\"track_id\").distinct()\n",
    "\n",
    "# Convert to a list for broadcast\n",
    "ranked_track_ids = [row.track_id for row in ranked_track_ids_df.collect() if row.track_id is not None]\n",
    "ranked_track_ids_broadcast = spark.sparkContext.broadcast(set(ranked_track_ids))\n",
    "\n",
    "# Define a UDF to check if a track ID is in the ranked set\n",
    "def is_in_rankings(track_id, ranked_ids_broadcast):\n",
    "    if track_id is None:\n",
    "        return False\n",
    "    return track_id in ranked_ids_broadcast.value\n",
    "\n",
    "# Register the UDF\n",
    "from pyspark.sql.types import BooleanType\n",
    "is_ranked_udf = udf(lambda x: is_in_rankings(x, ranked_track_ids_broadcast), BooleanType())\n",
    "\n",
    "# Add label column (1 if song is in rank_df, 0 otherwise)\n",
    "labeled_songs_df = filtered_songs_df.withColumn(\n",
    "    \"is_ranked\", \n",
    "    when(is_ranked_udf(col(\"id\")), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Count ranked vs. non-ranked songs\n",
    "ranked_count = labeled_songs_df.filter(col(\"is_ranked\") == 1).count()\n",
    "total_count = labeled_songs_df.count()\n",
    "\n",
    "print(f\"Songs that appeared in rankings: {ranked_count}\")\n",
    "print(f\"Songs that did not appear in rankings: {total_count - ranked_count}\")\n",
    "print(f\"Percentage of songs in rankings: {(ranked_count / total_count * 100):.2f}%\")\n",
    "\n",
    "# Cache the labeled data for subsequent operations\n",
    "labeled_songs_df = labeled_songs_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d20d4a37-6fe7-40ad-ab07-e6eee188eeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PREPARING FEATURES FOR CLASSIFICATION =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col, isnan, when\n",
    "\n",
    "# Step 6: Prepare features for classification\n",
    "print(\"\\n===== PREPARING FEATURES FOR CLASSIFICATION =====\")\n",
    "\n",
    "# Feature mapping from the original code\n",
    "feature_mapping = {\n",
    "    'danceability': 'af_danceability',\n",
    "    'energy': 'af_energy',\n",
    "    'key': 'af_key',\n",
    "    'loudness': 'af_loudness',\n",
    "    'mode': 'af_mode',\n",
    "    'speechiness': 'af_speechiness',\n",
    "    'acousticness': 'af_acousticness',\n",
    "    'instrumentalness': 'af_instrumentalness',\n",
    "    'liveness': 'af_liveness',\n",
    "    'valence': 'af_valence',\n",
    "    'tempo': 'af_tempo',\n",
    "    'time_signature': 'af_time_signature'\n",
    "}\n",
    "\n",
    "# Cast all feature columns to double\n",
    "for orig_col, target_col in feature_mapping.items():\n",
    "    filtered_songs_df = filtered_songs_df.withColumn(\n",
    "        orig_col, \n",
    "        col(orig_col).cast(\"double\")\n",
    "    )\n",
    "\n",
    "# List of feature columns to use\n",
    "feature_cols = list(feature_mapping.keys())\n",
    "\n",
    "# Create a feature vector column using VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Apply the vector assembler\n",
    "songs_with_features = assembler.transform(filtered_songs_df)\n",
    "\n",
    "# Normalize/Standardize the features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Fit the scaler on the data\n",
    "scaler_model = scaler.fit(songs_with_features)\n",
    "\n",
    "# Apply the scaler to the data\n",
    "scaled_songs = scaler_model.transform(songs_with_features)\n",
    "\n",
    "# Cache the prepared data for model training\n",
    "scaled_songs = scaled_songs.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10754f0a-be43-4d7a-8ddc-be68e76cc31c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prepared dataset to: gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_prepared_data\n"
     ]
    }
   ],
   "source": [
    "# Save the prepared dataset to GCS\n",
    "output_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_prepared_data\"\n",
    "\n",
    "# Save as parquet format (efficient columnar storage)\n",
    "scaled_songs.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Saved prepared dataset to: {output_path}\")\n",
    "\n",
    "# Alternatively, if you need CSV format:\n",
    "# scaled_songs.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path + \"_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "894aae8c-9d84-4f9d-82b7-76fbf3e93afb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated dataset schema:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- album: string (nullable = true)\n",
      " |-- album_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- artist_ids: string (nullable = true)\n",
      " |-- track_number: string (nullable = true)\n",
      " |-- disc_number: string (nullable = true)\n",
      " |-- explicit: string (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: double (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: double (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- duration_ms: string (nullable = true)\n",
      " |-- time_signature: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- artists_lower: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaled_features: vector (nullable = true)\n",
      " |-- is_ranked: integer (nullable = false)\n",
      "\n",
      "\n",
      "Distribution of the target variable:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|is_ranked| count|\n",
      "+---------+------+\n",
      "|        0|271221|\n",
      "|        1|  7510|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated dataset saved to: gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_prepared_data_complete\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "# First, let's load the original deduplicated dataset to get the track IDs\n",
    "deduplicated_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_deduplicated_data.csv\"\n",
    "spotify_df = spark.read.option(\"header\", \"true\").csv(deduplicated_path, inferSchema=True)\n",
    "\n",
    "# Extract ranked track IDs from the original dataset\n",
    "ranked_track_ids = spotify_df.select(\"track_id\").distinct()\n",
    "\n",
    "# Add is_ranked column to our current dataset using left join\n",
    "completed_songs = scaled_songs.join(\n",
    "    ranked_track_ids,\n",
    "    scaled_songs.id == ranked_track_ids.track_id,\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"is_ranked\",\n",
    "    when(col(\"track_id\").isNotNull(), 1).otherwise(0)\n",
    ").drop(\"track_id\")  # Drop the redundant track_id column from the join\n",
    "\n",
    "# Cache the updated dataset\n",
    "completed_songs = completed_songs.cache()\n",
    "\n",
    "# Check the new schema\n",
    "print(\"\\nUpdated dataset schema:\")\n",
    "completed_songs.printSchema()\n",
    "\n",
    "# Verify the is_ranked column was added\n",
    "print(\"\\nDistribution of the target variable:\")\n",
    "completed_songs.groupBy(\"is_ranked\").count().orderBy(\"is_ranked\").show()\n",
    "\n",
    "# Save the updated dataset with the is_ranked column\n",
    "updated_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_prepared_data_complete\"\n",
    "completed_songs.write.mode(\"overwrite\").parquet(updated_path)\n",
    "\n",
    "print(f\"\\nUpdated dataset saved to: {updated_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8184d524-65c2-479c-b36f-3085a30f6356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758359e0-0b84-4eac-9d78-e803046af051",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd37fe8e-f0ce-4e4a-97bb-52ceb405de76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/05 23:03:33 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/04/05 23:03:33 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/04/05 23:03:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/04/05 23:03:33 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prepared dataset from GCS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 23:03:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize a new Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spotify Classification Modeling\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the path to the updated dataset\n",
    "data_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_prepared_data_complete\"\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading prepared dataset from GCS...\")\n",
    "completed_songs = spark.read.parquet(data_path)\n",
    "\n",
    "# Cache the dataset for faster operations\n",
    "completed_songs = completed_songs.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f2b37a-ddf6-4511-927f-c007f3954469",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 278731 rows and 28 columns\n",
      "\n",
      "Dataset schema:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- album: string (nullable = true)\n",
      " |-- album_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- artist_ids: string (nullable = true)\n",
      " |-- track_number: string (nullable = true)\n",
      " |-- disc_number: string (nullable = true)\n",
      " |-- explicit: string (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: double (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: double (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- duration_ms: string (nullable = true)\n",
      " |-- time_signature: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- artists_lower: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaled_features: vector (nullable = true)\n",
      " |-- is_ranked: integer (nullable = true)\n",
      "\n",
      "\n",
      "Sample of the prepared data with target variable:\n",
      "+--------------------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|                  id|                name|             artists|is_ranked|            features|     scaled_features|\n",
      "+--------------------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|45hwi5isgnnozzB8N...|      Home Thru Hell|['The Flaming Lip...|        0|[0.54299999999999...|[0.11610756800988...|\n",
      "|7IPrX2pJUS18VpPiv...|One Thousand Sist...|['The Flaming Lip...|        0|[0.258,0.5,11.0,-...|[-1.3642878384820...|\n",
      "|0viRgU9QpNOKjKkDC...|         Shit Talkin|['The Flaming Lip...|        0|[0.28600000000000...|[-1.2188454827565...|\n",
      "|41nlO2k9QsazuKRGg...|      Hope Hell High|['The Flaming Lip...|        0|[0.517,0.7,7.0,-5...|[-0.0189460480209...|\n",
      "|7pOXg1vJs4IYqD5kB...|Motherfuckers Got...|['The Flaming Lip...|        0|[0.648,0.88599999...|[0.66151640198060...|\n",
      "+--------------------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(f\"\\nDataset size: {completed_songs.count()} rows and {len(completed_songs.columns)} columns\")\n",
    "\n",
    "# Show the schema\n",
    "print(\"\\nDataset schema:\")\n",
    "completed_songs.printSchema()\n",
    "\n",
    "# Show a sample of the data\n",
    "print(\"\\nSample of the prepared data with target variable:\")\n",
    "completed_songs.select(\"id\", \"name\", \"artists\", \"is_ranked\", \"features\", \"scaled_features\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f7ef3ca-8a7c-444b-8be4-ca085c971088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SPLITTING DATA INTO TRAIN AND TEST SETS =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 223011 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 55720 rows\n",
      "\n",
      "XGBoost for Spark not found. Using LogisticRegression and RandomForest instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING LOGISTIC REGRESSION MODEL =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.7940\n",
      "\n",
      "===== TRAINING RANDOM FOREST MODEL =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 23:05:20 WARN DAGScheduler: Broadcasting large task binary with size 1004.7 KiB\n",
      "25/04/05 23:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1851.2 KiB\n",
      "25/04/05 23:05:32 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/04/05 23:05:40 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/04/05 23:05:48 WARN DAGScheduler: Broadcasting large task binary with size 1580.2 KiB\n",
      "25/04/05 23:05:50 WARN DAGScheduler: Broadcasting large task binary with size 10.5 MiB\n",
      "25/04/05 23:06:00 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/04/05 23:06:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC: 0.8138\n",
      "\n",
      "Random Forest performed better!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 23:06:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Accuracy: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 23:06:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/04/05 23:06:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/04/05 23:06:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/04/05 23:06:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "True Positives: 0\n",
      "False Positives: 0\n",
      "True Negatives: 54275\n",
      "False Negatives: 1445\n",
      "\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 23:06:14 WARN TaskSetManager: Stage 94 contains a task of very large size (2075 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest model saved to: gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_randomforest_model\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "completed_songs = completed_songs.withColumn(\"label\", col(\"is_ranked\").cast(\"double\"))\n",
    "\n",
    "print(\"\\n===== SPLITTING DATA INTO TRAIN AND TEST SETS =====\")\n",
    "train_data, test_data = completed_songs.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set size: {train_data.count()} rows\")\n",
    "print(f\"Test set size: {test_data.count()} rows\")\n",
    "\n",
    "train_data = train_data.cache()\n",
    "test_data = test_data.cache()\n",
    "\n",
    "try:\n",
    "    from sparkxgb import XGBoostClassifier\n",
    "    \n",
    "    xgb = XGBoostClassifier(\n",
    "        featuresCol=\"scaled_features\", \n",
    "        labelCol=\"label\",\n",
    "        numRound=100,\n",
    "        maxDepth=6,\n",
    "        eta=0.1,\n",
    "        objective=\"binary:logistic\",\n",
    "        evalMetric=\"auc\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n===== TRAINING XGBOOST MODEL =====\")\n",
    "    xgb_model = xgb.fit(train_data)\n",
    "    \n",
    "    predictions = xgb_model.transform(test_data)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        rawPredictionCol=\"probabilities\", \n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    \n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    print(f\"XGBoost AUC: {auc:.4f}\")\n",
    "    \n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f\"XGBoost Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    tp = predictions.filter((col(\"prediction\") == 1) & (col(\"label\") == 1)).count()\n",
    "    fp = predictions.filter((col(\"prediction\") == 1) & (col(\"label\") == 0)).count()\n",
    "    tn = predictions.filter((col(\"prediction\") == 0) & (col(\"label\") == 0)).count()\n",
    "    fn = predictions.filter((col(\"prediction\") == 0) & (col(\"label\") == 1)).count()\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nPrecision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    model_path = \"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_xgboost_model\"\n",
    "    xgb_model.write().overwrite().save(model_path)\n",
    "    print(f\"\\nXGBoost model saved to: {model_path}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nXGBoost for Spark not found. Using LogisticRegression and RandomForest instead.\")\n",
    "    \n",
    "    from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "    \n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"scaled_features\", \n",
    "        labelCol=\"label\",\n",
    "        maxIter=10,\n",
    "        regParam=0.01\n",
    "    )\n",
    "    \n",
    "    print(\"\\n===== TRAINING LOGISTIC REGRESSION MODEL =====\")\n",
    "    lr_model = lr.fit(train_data)\n",
    "    \n",
    "    lr_predictions = lr_model.transform(test_data)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        rawPredictionCol=\"rawPrediction\", \n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    \n",
    "    lr_auc = evaluator.evaluate(lr_predictions)\n",
    "    print(f\"Logistic Regression AUC: {lr_auc:.4f}\")\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"scaled_features\", \n",
    "        labelCol=\"label\",\n",
    "        numTrees=100,\n",
    "        maxDepth=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\n===== TRAINING RANDOM FOREST MODEL =====\")\n",
    "    rf_model = rf.fit(train_data)\n",
    "    \n",
    "    rf_predictions = rf_model.transform(test_data)\n",
    "    \n",
    "    rf_auc = evaluator.evaluate(rf_predictions)\n",
    "    print(f\"Random Forest AUC: {rf_auc:.4f}\")\n",
    "    \n",
    "    if rf_auc > lr_auc:\n",
    "        print(\"\\nRandom Forest performed better!\")\n",
    "        best_model = rf_model\n",
    "        best_predictions = rf_predictions\n",
    "        best_model_name = \"RandomForest\"\n",
    "    else:\n",
    "        print(\"\\nLogistic Regression performed better!\")\n",
    "        best_model = lr_model\n",
    "        best_predictions = lr_predictions\n",
    "        best_model_name = \"LogisticRegression\"\n",
    "\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    best_accuracy = accuracy_evaluator.evaluate(best_predictions)\n",
    "    print(f\"Best Model Accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    tp = best_predictions.filter((col(\"prediction\") == 1) & (col(\"label\") == 1)).count()\n",
    "    fp = best_predictions.filter((col(\"prediction\") == 1) & (col(\"label\") == 0)).count()\n",
    "    tn = best_predictions.filter((col(\"prediction\") == 0) & (col(\"label\") == 0)).count()\n",
    "    fn = best_predictions.filter((col(\"prediction\") == 0) & (col(\"label\") == 1)).count()\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nPrecision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    model_path = f\"gs://dataproc-staging-us-central1-361128386781-eo9ksqfa/spotify_{best_model_name.lower()}_model\"\n",
    "    best_model.write().overwrite().save(model_path)\n",
    "    print(f\"\\n{best_model_name} model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61ce28c-78b2-4689-955f-a9168d36669b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING GRADIENT BOOSTED TREES MODEL =====\n",
      "GBT AUC: 0.8076\n",
      "\n",
      "===== TRAINING DECISION TREE MODEL =====\n",
      "Decision Tree AUC: 0.5000\n",
      "\n",
      "===== TRAINING LINEAR SVM MODEL =====\n",
      "Linear SVM AUC: 0.7744\n",
      "\n",
      "===== TRAINING NAIVE BAYES MODEL =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.9741\n",
      "\n",
      "===== TRAINING NEURAL NETWORK MODEL =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy: 0.9613\n",
      "\n",
      "===== ENSEMBLE MODEL VOTING =====\n",
      "Ensemble Model Accuracy: 0.9741\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, DecisionTreeClassifier, LinearSVC, NaiveBayes, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "print(\"\\n===== TRAINING GRADIENT BOOSTED TREES MODEL =====\")\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"label\",\n",
    "    maxIter=10,\n",
    "    maxDepth=5\n",
    ")\n",
    "gbt_model = gbt.fit(train_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "gbt_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\").evaluate(gbt_predictions)\n",
    "print(f\"GBT AUC: {gbt_auc:.4f}\")\n",
    "\n",
    "print(\"\\n===== TRAINING DECISION TREE MODEL =====\")\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"label\",\n",
    "    maxDepth=5\n",
    ")\n",
    "dt_model = dt.fit(train_data)\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "dt_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\").evaluate(dt_predictions)\n",
    "print(f\"Decision Tree AUC: {dt_auc:.4f}\")\n",
    "\n",
    "print(\"\\n===== TRAINING LINEAR SVM MODEL =====\")\n",
    "svm = LinearSVC(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"label\",\n",
    "    maxIter=10,\n",
    "    regParam=0.1\n",
    ")\n",
    "svm_model = svm.fit(train_data)\n",
    "svm_predictions = svm_model.transform(test_data)\n",
    "svm_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\").evaluate(svm_predictions)\n",
    "print(f\"Linear SVM AUC: {svm_auc:.4f}\")\n",
    "\n",
    "print(\"\\n===== TRAINING NAIVE BAYES MODEL =====\")\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"minmax_features\")\n",
    "minmax_model = minmax_scaler.fit(train_data)\n",
    "train_data_minmax = minmax_model.transform(train_data)\n",
    "test_data_minmax = minmax_model.transform(test_data)\n",
    "\n",
    "nb = NaiveBayes(\n",
    "    featuresCol=\"minmax_features\", \n",
    "    labelCol=\"label\",\n",
    "    modelType=\"multinomial\"\n",
    ")\n",
    "nb_model = nb.fit(train_data_minmax)\n",
    "nb_predictions = nb_model.transform(test_data_minmax)\n",
    "nb_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nb_accuracy = nb_evaluator.evaluate(nb_predictions)\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n===== TRAINING NEURAL NETWORK MODEL =====\")\n",
    "feature_size = len(train_data.select(\"scaled_features\").first()[0])\n",
    "layers = [feature_size, 10, 5, 2]\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"label\",\n",
    "    layers=layers,\n",
    "    blockSize=128,\n",
    "    seed=42,\n",
    "    maxIter=100\n",
    ")\n",
    "mlp_model = mlp.fit(train_data)\n",
    "mlp_predictions = mlp_model.transform(test_data)\n",
    "mlp_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\").evaluate(mlp_predictions)\n",
    "print(f\"Neural Network Accuracy: {mlp_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n===== ENSEMBLE MODEL VOTING =====\")\n",
    "from pyspark.sql.functions import col, when, greatest\n",
    "\n",
    "ensemble_df = test_data.select(\"id\", \"label\")\n",
    "ensemble_df = ensemble_df.join(gbt_predictions.select(\"id\", col(\"prediction\").alias(\"gbt_pred\")), \"id\")\n",
    "ensemble_df = ensemble_df.join(dt_predictions.select(\"id\", col(\"prediction\").alias(\"dt_pred\")), \"id\")\n",
    "ensemble_df = ensemble_df.join(svm_predictions.select(\"id\", col(\"prediction\").alias(\"svm_pred\")), \"id\")\n",
    "ensemble_df = ensemble_df.join(mlp_predictions.select(\"id\", col(\"prediction\").alias(\"mlp_pred\")), \"id\")\n",
    "\n",
    "ensemble_df = ensemble_df.withColumn(\n",
    "    \"vote_sum\", \n",
    "    col(\"gbt_pred\") + col(\"dt_pred\") + col(\"svm_pred\") + col(\"mlp_pred\")\n",
    ")\n",
    "ensemble_df = ensemble_df.withColumn(\n",
    "    \"ensemble_pred\", \n",
    "    when(col(\"vote_sum\") >= 2, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "ensemble_accuracy = ensemble_df.filter(col(\"ensemble_pred\") == col(\"label\")).count() / ensemble_df.count()\n",
    "print(f\"Ensemble Model Accuracy: {ensemble_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52999892-59ec-4159-a5e3-bf1af70dddde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EVALUATING ALL MODELS ON TEST DATA =====\n",
      "\n",
      "===== Gradient Boosted Trees EVALUATION =====\n",
      "Accuracy: 0.9613\n",
      "AUC: 0.8076\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Specificity: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives: 0\n",
      "False Positives: 1\n",
      "True Negatives: 54274\n",
      "False Negatives: 1445\n",
      "\n",
      "===== Decision Tree EVALUATION =====\n",
      "Accuracy: 0.9613\n",
      "AUC: 0.5000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Specificity: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives: 0\n",
      "False Positives: 0\n",
      "True Negatives: 54275\n",
      "False Negatives: 1445\n",
      "\n",
      "===== Linear SVM EVALUATION =====\n",
      "Accuracy: 0.9613\n",
      "AUC: 0.7744\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Specificity: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives: 0\n",
      "False Positives: 0\n",
      "True Negatives: 54275\n",
      "False Negatives: 1445\n",
      "\n",
      "===== Naive Bayes EVALUATION =====\n",
      "Accuracy: 0.9613\n",
      "AUC: 0.6118\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Specificity: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives: 0\n",
      "False Positives: 0\n",
      "True Negatives: 54275\n",
      "False Negatives: 1445\n",
      "\n",
      "===== Neural Network EVALUATION =====\n",
      "Accuracy: 0.9613\n",
      "AUC: 0.8238\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Specificity: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives: 0\n",
      "False Positives: 0\n",
      "True Negatives: 54275\n",
      "False Negatives: 1445\n",
      "\n",
      "===== Ensemble Model EVALUATION =====\n",
      "Accuracy: 0.9613\n",
      "AUC: Not available (no rawPrediction column)\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Specificity: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives: 0\n",
      "False Positives: 0\n",
      "True Negatives: 54275\n",
      "False Negatives: 1445\n",
      "\n",
      "===== MODEL COMPARISON (SORTED BY F1 SCORE) =====\n",
      "Model                Accuracy   AUC        Precision  Recall     F1 Score   Specificity\n",
      "--------------------------------------------------------------------------------\n",
      "Gradient Boosted Trees 0.9613     0.8076     0.0000     0.0000     0.0000     1.0000    \n",
      "Decision Tree        0.9613     0.5000     0.0000     0.0000     0.0000     1.0000    \n",
      "Linear SVM           0.9613     0.7744     0.0000     0.0000     0.0000     1.0000    \n",
      "Naive Bayes          0.9613     0.6118     0.0000     0.0000     0.0000     1.0000    \n",
      "Neural Network       0.9613     0.8238     0.0000     0.0000     0.0000     1.0000    \n",
      "Ensemble Model       0.9613     N/A        0.0000     0.0000     0.0000     1.0000    \n",
      "\n",
      "Best model by F1 score: Gradient Boosted Trees\n",
      "\n",
      "False Positive Examples (songs predicted to rank but didn't): 1\n",
      "+----------------------+-------------+------------------+\n",
      "|id                    |name         |artists           |\n",
      "+----------------------+-------------+------------------+\n",
      "|6ib2yZFSXu3DFbd89vxMlN|Singing on TV|['Little Warrior']|\n",
      "+----------------------+-------------+------------------+\n",
      "\n",
      "\n",
      "False Negative Examples (songs that ranked but predicted not to): 1445\n",
      "+----------------------+----------------------------------------------+------------------------------------------------------------+\n",
      "|id                    |name                                          |artists                                                     |\n",
      "+----------------------+----------------------------------------------+------------------------------------------------------------+\n",
      "|00R0fEFZGb5hyTgF1nrRCq|Look Over Your Shoulder (feat. Kendrick Lamar)|['Busta Rhymes', 'Kendrick Lamar']                          |\n",
      "|00S4PrcWUb3f3dQ9PuC0uy|2U                                            |['KANG DANIEL']                                             |\n",
      "|00bWqt93aqLXqKtzZoq7Jw|MOOO!                                         |['Doja Cat']                                                |\n",
      "|01TreyTchXP0J1Mn6wcVHt|Up Now (feat. G-Eazy and Rich The Kid)        |['Saweetie', 'London On Da Track', 'G-Eazy', 'Rich The Kid']|\n",
      "|01WDJ9gcaXFuNzY5ir9CaE|Lam Moi Toi Ngoi Khong                        |['Hoang Thuy Linh']                                         |\n",
      "+----------------------+----------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Prediction Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|55719|\n",
      "|       1.0|    1|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "Feature analysis for correct vs. incorrect predictions:\n",
      "danceability: Correct avg = 0.5169, Incorrect avg = 0.6555, Difference = 0.1386\n",
      "energy: Correct avg = 0.5498, Incorrect avg = 0.6404, Difference = 0.0905\n",
      "acousticness: Correct avg = 0.3819, Incorrect avg = 0.2247, Difference = 0.1571\n",
      "valence: Correct avg = 0.4051, Incorrect avg = 0.4688, Difference = 0.0637\n",
      "tempo: Correct avg = 118.8878, Incorrect avg = 121.4267, Difference = 2.5388\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, expr\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a function to calculate and display comprehensive metrics\n",
    "def evaluate_model(predictions, label_col=\"label\", prediction_col=\"prediction\", model_name=\"Model\", has_raw_prediction=True):\n",
    "    # Calculate basic metrics\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Area under ROC - only if the model has rawPrediction column\n",
    "    if has_raw_prediction:\n",
    "        binary_evaluator = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\")\n",
    "        auc = binary_evaluator.evaluate(predictions)\n",
    "    else:\n",
    "        auc = None  # Mark as unavailable\n",
    "    \n",
    "    # Calculate confusion matrix elements\n",
    "    tp = predictions.filter((col(prediction_col) == 1.0) & (col(label_col) == 1.0)).count()\n",
    "    fp = predictions.filter((col(prediction_col) == 1.0) & (col(label_col) == 0.0)).count()\n",
    "    tn = predictions.filter((col(prediction_col) == 0.0) & (col(label_col) == 0.0)).count()\n",
    "    fn = predictions.filter((col(prediction_col) == 0.0) & (col(label_col) == 1.0)).count()\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n===== {model_name} EVALUATION =====\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "    else:\n",
    "        print(\"AUC: Not available (no rawPrediction column)\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    \n",
    "    # Return metrics as a dictionary\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"auc\": auc if auc is not None else 0,  # Use 0 for sorting purposes\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"specificity\": specificity,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"tn\": tn,\n",
    "        \"fn\": fn\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\n===== EVALUATING ALL MODELS ON TEST DATA =====\")\n",
    "\n",
    "# Evaluate GBT\n",
    "gbt_metrics = evaluate_model(gbt_predictions, model_name=\"Gradient Boosted Trees\")\n",
    "\n",
    "# Evaluate Decision Tree\n",
    "dt_metrics = evaluate_model(dt_predictions, model_name=\"Decision Tree\")\n",
    "\n",
    "# Evaluate SVM\n",
    "svm_metrics = evaluate_model(svm_predictions, model_name=\"Linear SVM\")\n",
    "\n",
    "# Evaluate Naive Bayes\n",
    "nb_metrics = evaluate_model(nb_predictions, model_name=\"Naive Bayes\")\n",
    "\n",
    "# Evaluate Neural Network\n",
    "mlp_metrics = evaluate_model(mlp_predictions, model_name=\"Neural Network\")\n",
    "\n",
    "# Evaluate Ensemble - specify that it doesn't have rawPrediction column\n",
    "ensemble_metrics = evaluate_model(\n",
    "    ensemble_df, \n",
    "    label_col=\"label\", \n",
    "    prediction_col=\"ensemble_pred\", \n",
    "    model_name=\"Ensemble Model\",\n",
    "    has_raw_prediction=False\n",
    ")\n",
    "\n",
    "# Compare all models\n",
    "metrics_list = [gbt_metrics, dt_metrics, svm_metrics, nb_metrics, mlp_metrics, ensemble_metrics]\n",
    "metrics_list.sort(key=lambda x: x[\"f1\"], reverse=True)\n",
    "\n",
    "print(\"\\n===== MODEL COMPARISON (SORTED BY F1 SCORE) =====\")\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'AUC':<10} {'Precision':<10} {'Recall':<10} {'F1 Score':<10} {'Specificity':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for m in metrics_list:\n",
    "    auc_str = f\"{m['auc']:.4f}\" if m['auc'] != 0 else \"N/A\"\n",
    "    print(f\"{m['model']:<20} {m['accuracy']:<10.4f} {auc_str:<10} {m['precision']:<10.4f} {m['recall']:<10.4f} {m['f1']:<10.4f} {m['specificity']:<10.4f}\")\n",
    "\n",
    "# Identify best model by F1 score\n",
    "best_model = metrics_list[0][\"model\"]\n",
    "print(f\"\\nBest model by F1 score: {best_model}\")\n",
    "\n",
    "# Map model names to dataframes for error analysis\n",
    "model_predictions = {\n",
    "    \"Gradient Boosted Trees\": gbt_predictions,\n",
    "    \"Decision Tree\": dt_predictions,\n",
    "    \"Linear SVM\": svm_predictions,\n",
    "    \"Naive Bayes\": nb_predictions,\n",
    "    \"Neural Network\": mlp_predictions,\n",
    "    \"Ensemble Model\": ensemble_df\n",
    "}\n",
    "\n",
    "# Get the predictions for the best model\n",
    "predictions = model_predictions[best_model]\n",
    "prediction_col = \"ensemble_pred\" if best_model == \"Ensemble Model\" else \"prediction\"\n",
    "\n",
    "# Get false positives (songs predicted to rank but didn't)\n",
    "false_positives = predictions.filter((col(prediction_col) == 1.0) & (col(\"label\") == 0.0))\n",
    "print(f\"\\nFalse Positive Examples (songs predicted to rank but didn't): {false_positives.count()}\")\n",
    "if \"name\" in predictions.columns and \"artists\" in predictions.columns:\n",
    "    false_positives.select(\"id\", \"name\", \"artists\").show(5, truncate=False)\n",
    "else:\n",
    "    false_positives.select(\"id\").show(5)\n",
    "\n",
    "# Get false negatives (songs that ranked but predicted not to)\n",
    "false_negatives = predictions.filter((col(prediction_col) == 0.0) & (col(\"label\") == 1.0))\n",
    "print(f\"\\nFalse Negative Examples (songs that ranked but predicted not to): {false_negatives.count()}\")\n",
    "if \"name\" in predictions.columns and \"artists\" in predictions.columns:\n",
    "    false_negatives.select(\"id\", \"name\", \"artists\").show(5, truncate=False)\n",
    "else:\n",
    "    false_negatives.select(\"id\").show(5)\n",
    "\n",
    "# Check prediction distribution\n",
    "prediction_dist = predictions.groupBy(prediction_col).count().orderBy(prediction_col)\n",
    "print(\"\\nPrediction Distribution:\")\n",
    "prediction_dist.show()\n",
    "\n",
    "# Check feature distributions if features exist in the predictions dataframe\n",
    "if all(col in predictions.columns for col in [\"danceability\", \"energy\", \"acousticness\", \"valence\", \"tempo\"]):\n",
    "    print(\"\\nFeature analysis for correct vs. incorrect predictions:\")\n",
    "    feature_cols = [\"danceability\", \"energy\", \"acousticness\", \"valence\", \"tempo\"]\n",
    "\n",
    "    for feature in feature_cols:\n",
    "        correct_predictions = predictions.filter(col(prediction_col) == col(\"label\"))\n",
    "        incorrect_predictions = predictions.filter(col(prediction_col) != col(\"label\"))\n",
    "        \n",
    "        correct_avg = correct_predictions.select(expr(f\"avg({feature})\")).collect()[0][0]\n",
    "        incorrect_avg = incorrect_predictions.select(expr(f\"avg({feature})\")).collect()[0][0]\n",
    "        \n",
    "        print(f\"{feature}: Correct avg = {correct_avg:.4f}, Incorrect avg = {incorrect_avg:.4f}, Difference = {abs(correct_avg - incorrect_avg):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39ac143a-3a21-4831-a42a-a1c203b6cef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
